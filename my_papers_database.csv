title,authors,abstract,published_date,year,categories,pdf_url,arxiv_id,source,citation_count,journal
How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks,"Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir","Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.",2025-07-02,2025,"cs.CV, cs.AI, cs.LG",http://arxiv.org/pdf/2507.01955v1,2507.01955v1,arxiv,0,arXiv preprint
Test-Time Scaling with Reflective Generative Model,"Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie","We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",2025-07-02,2025,"cs.LG, cs.CL",http://arxiv.org/pdf/2507.01951v1,2507.01951v1,arxiv,0,arXiv preprint
Kwai Keye-VL Technical Report,"Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang","While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01949v1,2507.01949v1,arxiv,0,arXiv preprint
Deep BSVIEs Parametrization and Learning-Based Applications,"Nacira Agram, Giulia Pucci","We study the numerical approximation of backward stochastic Volterra integral
equations (BSVIEs) and their reflected extensions, which naturally arise in
problems with time inconsistency, path dependent preferences, and recursive
utilities with memory. These equations generalize classical BSDEs by involving
two dimensional time structures and more intricate dependencies.
  We begin by developing a well posedness and measurability framework for
BSVIEs in product probability spaces. Our approach relies on a representation
of the solution as a parametrized family of backward stochastic equations
indexed by the initial time, and draws on results of Stricker and Yor to ensure
that the two parameter solution is well defined in a joint measurable sense.
  We then introduce a discrete time learning scheme based on a recursive
backward representation of the BSVIE, combining the discretization of Hamaguchi
and Taguchi with deep neural networks. A detailed convergence analysis is
provided, generalizing the framework of deep BSDE solvers to the two
dimensional BSVIE setting. Finally, we extend the solver to reflected BSVIEs,
motivated by applications in delayed recursive utility with lower constraints.",2025-07-02,2025,math.PR,http://arxiv.org/pdf/2507.01948v1,2507.01948v1,arxiv,0,arXiv preprint
Characterizing control between interacting subsystems with deep Jacobian estimation,"Adam J. Eisen, Mitchell Ostrow, Sarthak Chandra, Leo Kozachkov, Earl K. Miller, Ila R. Fiete","Biological function arises through the dynamical interactions of multiple
subsystems, including those between brain areas, within gene regulatory
networks, and more. A common approach to understanding these systems is to
model the dynamics of each subsystem and characterize communication between
them. An alternative approach is through the lens of control theory: how the
subsystems control one another. This approach involves inferring the
directionality, strength, and contextual modulation of control between
subsystems. However, methods for understanding subsystem control are typically
linear and cannot adequately describe the rich contextual effects enabled by
nonlinear complex systems. To bridge this gap, we devise a data-driven
nonlinear control-theoretic framework to characterize subsystem interactions
via the Jacobian of the dynamics. We address the challenge of learning
Jacobians from time-series data by proposing the JacobianODE, a deep learning
method that leverages properties of the Jacobian to directly estimate it for
arbitrary dynamical systems from data alone. We show that JacobianODEs
outperform existing Jacobian estimation methods on challenging systems,
including high-dimensional chaos. Applying our approach to a multi-area
recurrent neural network (RNN) trained on a working memory selection task, we
show that the ""sensory"" area gains greater control over the ""cognitive"" area
over learning. Furthermore, we leverage the JacobianODE to directly control the
trained RNN, enabling precise manipulation of its behavior. Our work lays the
foundation for a theoretically grounded and data-driven understanding of
interactions among biological subsystems.",2025-07-02,2025,"q-bio.QM, cs.LG, math.DS, q-bio.NC",http://arxiv.org/pdf/2507.01946v1,2507.01946v1,arxiv,0,arXiv preprint
ML-Driven Strong Lens Discoveries: Down to $θ_E \sim 0.03''$ and $M_\mathrm{halo}< 10^{11} M_\odot$,"Ethan Silver, R. Wang, Xiaosheng Huang, A. Bolton, C. Storfer, S. Banka","We present results on extending the strong lens discovery space down to much
smaller Einstein radii ($\theta_E\lesssim0.03''$) and much lower halo mass
($M_\mathrm{halo}<10^{11}M_\odot$) through the combination of JWST observations
and machine learning (ML) techniques. First, we forecast detectable strong
lenses with JWST using CosmoDC2 as the lens catalog, and a source catalog down
to 29th magnitude. By further incorporating the VELA hydrodynamical simulations
of high-redshift galaxies, we simulate strong lenses. We train a ResNet on
these images, achieving near-100\% completeness and purity for ``conventional""
strong lenses ($\theta_E\gtrsim 0.5''$), applicable to JWST, HST, the Roman
Space Telescope and Euclid VIS. For the first time, we also search for very low
halo mass strong lenses ($M_{halo}<10^{11}M_\odot$) in simulations, with
$\theta_E\ll 0.5''$, down to the best resolution ($0.03''$) and depth
(10,000~sec) limits of JWST using ResNet. A U-Net model is employed to pinpoint
these small lenses in images, which are otherwise virtually impossible for
human detection. Our results indicate that JWST can find $\sim 17$/deg$^2$ such
low-halo-mass lenses, with the locations of $\sim 1.1$/deg$^2$ of these
detectable by the U-Net at $\sim100$\% precision (and $\sim 7.0$/deg$^2$ at a
99.0\% precision). To validate our model for finding ``conventional"" strong
lenses, we apply it to HST images, discovering two new strong lens candidates
previously missed by human classifiers in a crowdsourcing project (Garvin et
al. 2022). This study demonstrates the (potentially ``superhuman"") advantages
of ML combined with current and future space telescopes for detecting
conventional, and especially, low-halo-mass strong lenses, which are critical
for testing CDM models.",2025-07-02,2025,"astro-ph.CO, astro-ph.GA",http://arxiv.org/pdf/2507.01943v1,2507.01943v1,arxiv,0,arXiv preprint
SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars,"Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo","In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.",2025-07-02,2025,"astro-ph.IM, astro-ph.SR, cs.AI, cs.LG",http://arxiv.org/pdf/2507.01939v1,2507.01939v1,arxiv,0,arXiv preprint
A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition,"Zhaosong Lu, Xiangyuan Wang","We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\""older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.",2025-07-02,2025,"math.OC, cs.LG, cs.NA, math.NA, stat.ML, 90C26, 90C30, 90C47, 90C99, 65K05",http://arxiv.org/pdf/2507.01932v1,2507.01932v1,arxiv,0,arXiv preprint
Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla,"Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman","In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.",2025-07-02,2025,"cs.CL, cs.AI, cs.SD, eess.AS",http://arxiv.org/pdf/2507.01931v1,2507.01931v1,arxiv,0,arXiv preprint
IC-Custom: Diverse Image Customization via In-Context Learning,"Yaowei Li, Xiaoyu Li, Zhaoyang Zhang, Yuxuan Bian, Gan Liu, Xinyuan Li, Jiale Xu, Wenbo Hu, Yating Liu, Lingen Li, Jing Cai, Yuexian Zou, Yancheng He, Ying Shan","Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01926v1,2507.01926v1,arxiv,0,arXiv preprint
Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection,"Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari","The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.",2025-07-02,2025,"cs.LG, cs.AI",http://arxiv.org/pdf/2507.01924v1,2507.01924v1,arxiv,0,arXiv preprint
NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks,"Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li","Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality ""NaturalThoughts"" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.",2025-07-02,2025,cs.CL,http://arxiv.org/pdf/2507.01921v1,2507.01921v1,arxiv,0,arXiv preprint
End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning,"Christian Bongiorno, Efstratios Manolakis, Rosario Nunzio Mantegna","We develop a rotation-invariant neural network that provides the global
minimum-variance portfolio by jointly learning how to lag-transform historical
returns and how to regularise both the eigenvalues and the marginal
volatilities of large equity covariance matrices. This explicit mathematical
mapping offers clear interpretability of each module's role, so the model
cannot be regarded as a pure black-box. The architecture mirrors the analytical
form of the global minimum-variance solution yet remains agnostic to dimension,
so a single model can be calibrated on panels of a few hundred stocks and
applied, without retraining, to one thousand US equities-a cross-sectional jump
that demonstrates robust out-of-sample generalisation. The loss function is the
future realized minimum portfolio variance and is optimized end-to-end on real
daily returns. In out-of-sample tests from January 2000 to December 2024 the
estimator delivers systematically lower realised volatility, smaller maximum
drawdowns, and higher Sharpe ratios than the best analytical competitors,
including state-of-the-art non-linear shrinkage. Furthermore, although the
model is trained end-to-end to produce an unconstrained (long-short)
minimum-variance portfolio, we show that its learned covariance representation
can be used in general optimizers under long-only constraints with virtually no
loss in its performance advantage over competing estimators. These gains
persist when the strategy is executed under a highly realistic implementation
framework that models market orders at the auctions, empirical slippage,
exchange fees, and financing charges for leverage, and they remain stable
during episodes of acute market stress.",2025-07-02,2025,"q-fin.PM, cs.AI, math.OC, physics.data-an, stat.ML, 91G10 (Primary) 68T07, 91G60, 62P05 (Secondary), I.2.6; I.5.1; G.3; J.4",http://arxiv.org/pdf/2507.01918v1,2507.01918v1,arxiv,0,arXiv preprint
Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models,"Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He","Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.",2025-07-02,2025,"cs.CL, cs.AI, cs.LG",http://arxiv.org/pdf/2507.01915v1,2507.01915v1,arxiv,0,arXiv preprint
Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction,"Apoorv Verma, Junaid Jami, Amrita Bhattacharya","Accurately predicting magnetic behavior across diverse materials systems
remains a longstanding challenge due to the complex interplay of structural and
electronic factors and is pivotal for the accelerated discovery and design of
next-generation magnetic materials. In this work, a refined descriptor is
proposed that significantly improves the prediction of two critical magnetic
properties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic
moment per atom -- using only the structural information of materials. Unlike
previous models limited to Mn-based or lanthanide-transition metal compounds,
the present approach generalizes across a diverse dataset of 5741 stable,
binary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the
Materials Project. Leveraging an enriched elemental vector representation and
advanced feature engineering, including nonlinear terms and reduced matrix
sparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic
ordering classification and balanced recall across FM and FiM classes,
addressing a key limitation in prior studies. The model predicts magnetic
moment per atom with a correlation coefficient of 0.93, surpassing the Hund's
matrix and orbital field matrix descriptors. Additionally, it accurately
estimates formation energy per atom, enabling assessment of both magnetic
behavior and material stability. This generalized and computationally efficient
framework offers a robust tool for high-throughput screening of magnetic
materials with tailored properties.",2025-07-02,2025,"cond-mat.mtrl-sci, cs.LG",http://arxiv.org/pdf/2507.01913v1,2507.01913v1,arxiv,0,arXiv preprint
3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP,"Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee","In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01912v1,2507.01912v1,arxiv,0,arXiv preprint
High-Layer Attention Pruning with Rescaling,"Songtao Liu, Peng Liu","Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.",2025-07-02,2025,"cs.CL, cs.LG",http://arxiv.org/pdf/2507.01900v1,2507.01900v1,arxiv,0,arXiv preprint
STEM Diffraction Pattern Analysis with Deep Learning Networks,"Sebastian Wissel, Jonas Scheunert, Aaron Dextre, Shamail Ahmed, Andreas Bayer, Kerstin Volz, Bai-Xiang Xu","Accurate grain orientation mapping is essential for understanding and
optimizing the performance of polycrystalline materials, particularly in
energy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising
cathode material for next-generation lithium-ion batteries, and its
electrochemical behaviour is closely linked to microstructural features such as
grain size and crystallographic orientations. Traditional orientation mapping
methods--such as manual indexing, template matching (TM), or Hough
transform-based techniques--are often slow and noise-sensitive when handling
complex or overlapping patterns, creating a bottleneck in large-scale
microstructural analysis. This work presents a machine learning-based approach
for predicting Euler angles directly from scanning transmission electron
microscopy (STEM) diffraction patterns (DPs). This enables the automated
generation of high-resolution crystal orientation maps, facilitating the
analysis of internal microstructures at the nanoscale. Three deep learning
architectures--convolutional neural networks (CNNs), Dense Convolutional
Networks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,
using an experimentally acquired dataset labelled via a commercial TM
algorithm. While the CNN model serves as a baseline, both DenseNets and Swin
Transformers demonstrate superior performance, with the Swin Transformer
achieving the highest evaluation scores and the most consistent microstructural
predictions. The resulting crystal maps exhibit clear grain boundary
delineation and coherent intra-grain orientation distributions, underscoring
the potential of attention-based architectures for analyzing diffraction-based
image data. These findings highlight the promise of combining advanced machine
learning models with STEM data for robust, high-throughput microstructural
characterization.",2025-07-02,2025,"cond-mat.dis-nn, cond-mat.mtrl-sci, cs.LG",http://arxiv.org/pdf/2507.01889v1,2507.01889v1,arxiv,0,arXiv preprint
MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants,"Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou","Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
""SLMs Learnability Gap"". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.",2025-07-02,2025,cs.CL,http://arxiv.org/pdf/2507.01887v1,2507.01887v1,arxiv,0,arXiv preprint
Improving GANs by leveraging the quantum noise from real hardware,"Hongni Jin, Kenneth M. Merz Jr","We propose a novel approach to generative adversarial networks (GANs) in
which the standard i.i.d. Gaussian latent prior is replaced or hybridized with
a quantum-correlated prior derived from measurements of a 16-qubit entangling
circuit. Each latent sample is generated by grouping repeated shots per qubit
into a binary fraction, applying the inverse Gaussian CDF to obtain a
16-dimensional Gaussian vector whose joint copula reflects genuine quantum
entanglement, and then projecting into the high-dimensional space via a fixed
random matrix. By pre-sampling tens of millions of bitstrings, either from a
noiseless simulator or from IBM hardware, we build large pools of independent
but internally quantum-correlated latents. We integrate this prior into three
representative architectures (WGAN, SNGAN, BigGAN) on CIFAR-10, making no
changes to the neural network structure or training hyperparameters. The hybrid
latent representations incorporating hardware-derived noise consistently lower
the FID relative to both the classical baseline and the simulator variant,
especially when the quantum component constitutes a substantial fraction of the
prior. In addition, we execute on the QPU in parallel to not only save
computing time but also further decrease the FID up to 17% in BigGAN. These
results indicate that intrinsic quantum randomness and device-specific
imperfections can provide a structured inductive bias that enhances GAN
performance. Our work demonstrates a practical pipeline for leveraging noisy
quantum hardware to enrich deep-generative modeling, opening a new interface
between quantum information and machine learning. All code and data are
available at https://github.com/Neon8988/GAN_QN.git.",2025-07-02,2025,quant-ph,http://arxiv.org/pdf/2507.01886v1,2507.01886v1,arxiv,0,arXiv preprint
Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification,"Kunlun Xu, Fan Zhuo, Jiangmeng Li, Xu Zou, Jiahuan Zhou","Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01884v1,2507.01884v1,arxiv,0,arXiv preprint
Future Slot Prediction for Unsupervised Object Discovery in Surgical Video,"Guiqiu Liao, Matjaz Jogan, Marcel Hussing, Edward Zhang, Eric Eaton, Daniel A. Hashimoto","Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01882v1,2507.01882v1,arxiv,0,arXiv preprint
A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs,"Niccolò McConnell, Pardeep Vasudev, Daisuke Yamada, Daryl Cheng, Mehran Azimbagirad, John McCabe, Shahab Aslani, Ahmed H. Shahin, Yukun Zhou, The SUMMIT Consortium, Andre Altmann, Yipeng Hu, Paul Taylor, Sam M. Janes, Daniel C. Alexander, Joseph Jacob","Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.",2025-07-02,2025,"eess.IV, cs.CV, cs.LG",http://arxiv.org/pdf/2507.01881v1,2507.01881v1,arxiv,0,arXiv preprint
Evolving HPC services to enable ML workloads on HPE Cray EX,"Stefano Schuppli, Fawzi Mohamed, Henrique Mendonça, Nina Mujkanovic, Elia Palme, Dino Conciatore, Lukas Drescher, Miguel Gila, Pim Witlox, Joost VandeVondele, Maxime Martinasso, Thomas C. Schulthess, Torsten Hoefler","The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.",2025-07-02,2025,"cs.DC, cs.LG",http://arxiv.org/pdf/2507.01880v1,2507.01880v1,arxiv,0,arXiv preprint
Towards Foundation Auto-Encoders for Time-Series Anomaly Detection,"Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández","We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.",2025-07-02,2025,"cs.LG, cs.AI",http://arxiv.org/pdf/2507.01875v1,2507.01875v1,arxiv,0,arXiv preprint
DIY-MKG: An LLM-Based Polyglot Language Learning System,"Kenan Tang, Yanhong Li, Yao Qin","Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.",2025-07-02,2025,cs.CL,http://arxiv.org/pdf/2507.01872v1,2507.01872v1,arxiv,0,arXiv preprint
Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition,"Anders Aamand, Justin Y. Chen, Mina Dalirrooyfard, Slobodan Mitrović, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu","We study differentially private algorithms for graph cut sparsification, a
fundamental problem in algorithms, privacy, and machine learning. While
significant progress has been made, the best-known private and efficient cut
sparsifiers on $n$-node graphs approximate each cut within
$\widetilde{O}(n^{1.5})$ additive error and $1+\gamma$ multiplicative error for
any $\gamma > 0$ [Gupta, Roth, Ullman TCC'12]. In contrast, ""inefficient""
algorithms, i.e., those requiring exponential time, can achieve an
$\widetilde{O}(n)$ additive error and $1+\gamma$ multiplicative error
[Eli{\'a}{\v{s}}, Kapralov, Kulkarni, Lee SODA'20]. In this work, we break the
$n^{1.5}$ additive error barrier for private and efficient cut sparsification.
We present an $(\varepsilon,\delta)$-DP polynomial time algorithm that, given a
non-negative weighted graph, outputs a private synthetic graph approximating
all cuts with multiplicative error $1+\gamma$ and additive error $n^{1.25 +
o(1)}$ (ignoring dependencies on $\varepsilon, \delta, \gamma$).
  At the heart of our approach lies a private algorithm for expander
decomposition, a popular and powerful technique in (non-private) graph
algorithms.",2025-07-02,2025,cs.DS,http://arxiv.org/pdf/2507.01873v1,2507.01873v1,arxiv,0,arXiv preprint
Direct Vertex Reconstruction of $Λ$ Baryons from Hits in CLAS12 using Graph Neural Networks,"Keegan Menkce, Matthew McEneaney, Anselm Vossen","Machine learning techniques, including Graph Neural Networks (GNNs), have
been used extensively for data analysis in high energy and nuclear physics.
Here we report on the use of a GNN to reconstruct decay vertices of $\Lambda$
hyperons directly from hits in the tracking detector at the CLAS12 experiment
at Jefferson Laboratory (JLab). We show that we can improve the vertex
reconstruction in simulation compared to the standard, track based, algorithm.
We believe this warrants further study. The current study is limited by
available training resources but points to an interesting possibility to forgo
vertex reconstruction by track fitting in a complicated magnetic field for a
more direct approach where the hit to vertex mapping is encoded in a neural
network.",2025-07-02,2025,hep-ex,http://arxiv.org/pdf/2507.01868v1,2507.01868v1,arxiv,0,arXiv preprint
An in-silico lung phantom to assess the performance of pulmonary artery segmentation using angiogram,"Sunder Neelakantan, Tanmay Mukherjee, Emilio A. Mendiola, Kyle Myers, Reza Avazmohammadi","Pulmonary hypertension (PH) can lead to significant vascular remodeling,
resulting in altered pulmonary blood flow. Estimating the patient-specific
contributions of each remodeling event is necessary to optimize and
individualize clinical intervention strategies. In-silico modeling has emerged
as a powerful tool to simulate pulmonary hemodynamics, and one of the primary
requirements for robust in-silico modeling is an accurate representation of the
pulmonary vasculature structure. Computed tomography (CT) imaging can be used
to segment and reconstruct the proximal vasculature. However, contrast-enhanced
imaging, such as CT pulmonary angiography, is required to obtain a
comprehensive and high-fidelity view of the pulmonary vasculature. The clinical
use of CT pulmonary angiography is limited by the complications associated with
the injection of contrast agents. Machine learning (ML) approaches have emerged
to effectively segment and reconstruct the pulmonary vasculature without the
need for contrast-enhanced imaging. We have developed a method to create
in-silico pulmonary angiogram phantoms with varying simulated contrast levels.
The results indicated that adding simulated contrast can allow for successful
segmentation of the pulmonary vasculature. We expect this method to assist with
developing and training ML-based segmentation frameworks and aid in their
validation, thereby improving the capability to segment and reconstruct
pulmonary vasculature without using contrast-enhanced imaging.",2025-07-02,2025,physics.bio-ph,http://arxiv.org/pdf/2507.01867v1,2507.01867v1,arxiv,0,arXiv preprint
TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types,"Yuhao Lin, Yi-Lin Wei, Haoran Liao, Mu Lin, Chengyi Xing, Hao Li, Dandan Zhang, Mark Cutkosky, Wei-Shi Zheng","Dexterous teleoperation plays a crucial role in robotic manipulation for
real-world data collection and remote robot control. Previous dexterous
teleoperation mostly relies on hand retargeting to closely mimic human hand
postures. However, these approaches may fail to fully leverage the inherent
dexterity of dexterous hands, which can execute unique actions through their
structural advantages compared to human hands. To address this limitation, we
propose TypeTele, a type-guided dexterous teleoperation system, which enables
dexterous hands to perform actions that are not constrained by human motion
patterns. This is achieved by introducing dexterous manipulation types into the
teleoperation system, allowing operators to employ appropriate types to
complete specific tasks. To support this system, we build an extensible
dexterous manipulation type library to cover comprehensive dexterous postures
used in manipulation tasks. During teleoperation, we employ a MLLM
(Multi-modality Large Language Model)-assisted type retrieval module to
identify the most suitable manipulation type based on the specific task and
operator commands. Extensive experiments of real-world teleoperation and
imitation learning demonstrate that the incorporation of manipulation types
significantly takes full advantage of the dexterous robot's ability to perform
diverse and complex tasks with higher success rates.",2025-07-02,2025,cs.RO,http://arxiv.org/pdf/2507.01857v1,2507.01857v1,arxiv,0,arXiv preprint
AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation,"Sixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang","Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.",2025-07-02,2025,"cs.RO, cs.AI",http://arxiv.org/pdf/2507.01961v1,2507.01961v1,arxiv,0,arXiv preprint
Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation,"Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, Song Han","We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.",2025-07-02,2025,"cs.CV, cs.AI",http://arxiv.org/pdf/2507.01957v1,2507.01957v1,arxiv,0,arXiv preprint
FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model,"Yukang Cao, Chenyang Si, Jinghao Wang, Ziwei Liu","We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01953v1,2507.01953v1,arxiv,0,arXiv preprint
Test-Time Scaling with Reflective Generative Model,"Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie","We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",2025-07-02,2025,"cs.LG, cs.CL",http://arxiv.org/pdf/2507.01951v1,2507.01951v1,arxiv,0,arXiv preprint
String Breaking Dynamics and Glueball Formation in a $2+1$D Lattice Gauge Theory,"Kaidi Xu, Umberto Borla, Sergej Moroz, Jad C. Halimeh","With the advent of advanced quantum processors capable of probing lattice
gauge theories (LGTs) in higher spatial dimensions, it is crucial to understand
string dynamics in such models to guide upcoming experiments and to make
connections to high-energy physics (HEP). Using tensor network methods, we
study the far-from-equilibrium quench dynamics of electric flux strings between
two static charges in the $2+1$D $\mathbb{Z}_2$ LGT with dynamical matter. We
calculate the probabilities of finding the time-evolved wave function in string
configurations of the same length as the initial string. At resonances
determined by the the electric field strength and the mass, we identify various
string breaking processes accompanied with matter creation. Away from resonance
strings exhibit intriguing confined dynamics which, for strong electric fields,
we fully characterize through effective perturbative models. Starting in
maximal-length strings, we find that the wave function enters a dynamical
regime where it splits into shorter strings and disconnected loops, with the
latter bearing qualitative resemblance to glueballs in quantum chromodynamics
(QCD). Our findings can be probed on state-of-the-art superconducting-qubit and
trapped-ion quantum processors.",2025-07-02,2025,"hep-lat, cond-mat.quant-gas, cond-mat.stat-mech, hep-th, quant-ph",http://arxiv.org/pdf/2507.01950v1,2507.01950v1,arxiv,0,arXiv preprint
Kwai Keye-VL Technical Report,"Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang","While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01949v1,2507.01949v1,arxiv,0,arXiv preprint
Deep BSVIEs Parametrization and Learning-Based Applications,"Nacira Agram, Giulia Pucci","We study the numerical approximation of backward stochastic Volterra integral
equations (BSVIEs) and their reflected extensions, which naturally arise in
problems with time inconsistency, path dependent preferences, and recursive
utilities with memory. These equations generalize classical BSDEs by involving
two dimensional time structures and more intricate dependencies.
  We begin by developing a well posedness and measurability framework for
BSVIEs in product probability spaces. Our approach relies on a representation
of the solution as a parametrized family of backward stochastic equations
indexed by the initial time, and draws on results of Stricker and Yor to ensure
that the two parameter solution is well defined in a joint measurable sense.
  We then introduce a discrete time learning scheme based on a recursive
backward representation of the BSVIE, combining the discretization of Hamaguchi
and Taguchi with deep neural networks. A detailed convergence analysis is
provided, generalizing the framework of deep BSDE solvers to the two
dimensional BSVIE setting. Finally, we extend the solver to reflected BSVIEs,
motivated by applications in delayed recursive utility with lower constraints.",2025-07-02,2025,math.PR,http://arxiv.org/pdf/2507.01948v1,2507.01948v1,arxiv,0,arXiv preprint
SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars,"Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo","In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.",2025-07-02,2025,"astro-ph.IM, astro-ph.SR, cs.AI, cs.LG",http://arxiv.org/pdf/2507.01939v1,2507.01939v1,arxiv,0,arXiv preprint
The Roper Resonance $N^*(1440)$ in Nucleon-Nucleon Collisions and the Issue of Dibaryons,Heinz Clement,"In many reactions leading to excitations of the nucleon the Roper resonance
$N^*(1440)$ can be sensed only by complex partial-wave analyses. In
nucleon-nucleon collisions the isoscalar single-pion production as well as
specific two-pion production channels present the Roper excitation free of
competing resonance processes at a mass of 1370 MeV and a width of 150 MeV. A
detailed analysis points to the formation of $N^*(1440)N$ dibaryonic systems
during the nucleon-nucleon collision process similar to what is known from the
$\Delta(1232)N$ threshold.",2025-07-02,2025,"hep-ex, nucl-ex, nucl-th",http://arxiv.org/pdf/2507.01937v1,2507.01937v1,arxiv,0,arXiv preprint
The Thin Line Between Comprehension and Persuasion in LLMs,"Adrian de Wynter, Tangming Yuan","Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.",2025-07-02,2025,"cs.CL, cs.CY",http://arxiv.org/pdf/2507.01936v1,2507.01936v1,arxiv,0,arXiv preprint
Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla,"Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman","In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.",2025-07-02,2025,"cs.CL, cs.AI, cs.SD, eess.AS",http://arxiv.org/pdf/2507.01931v1,2507.01931v1,arxiv,0,arXiv preprint
Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations,"Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan","Large Language Models (LLMs) have revolutionized robotic autonomy, including
Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential
of LLMs for translating human instructions into executable control code for UAV
operations. However, LLMs still face challenges from logical reasoning and
complex decision-making, leading to concerns about the reliability of
LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop
control framework that enables reliable UAV operations powered by effective
feedback and refinement using two LLM modules, i.e., a Code Generator and an
Evaluator. Our framework transforms numerical state observations from UAV
operations into natural language trajectory descriptions to enhance the
evaluator LLM's understanding of UAV dynamics for precise feedback generation.
Our framework also enables a simulation-based refinement process, and hence
eliminates the risks to physical UAVs caused by incorrect code execution during
the refinement. Extensive experiments on UAV control tasks with different
complexities are conducted. The experimental results show that our framework
can achieve reliable UAV operations using LLMs, which significantly outperforms
baseline approaches in terms of success rate and completeness with the increase
of task complexity.",2025-07-02,2025,cs.RO,http://arxiv.org/pdf/2507.01930v1,2507.01930v1,arxiv,0,arXiv preprint
evMLP: An Efficient Event-Driven MLP Architecture for Vision,Zhentan Zheng,"Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as ""events"". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01927v1,2507.01927v1,arxiv,0,arXiv preprint
A Survey on Vision-Language-Action Models: An Action Tokenization Perspective,"Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang","The remarkable advancements of vision and language foundation models in
multimodal understanding, reasoning, and generation has sparked growing efforts
to extend such intelligence to the physical world, fueling the flourishing of
vision-language-action (VLA) models. Despite seemingly diverse approaches, we
observe that current VLA models can be unified under a single framework: vision
and language inputs are processed by a series of VLA modules, producing a chain
of \textit{action tokens} that progressively encode more grounded and
actionable information, ultimately generating executable actions. We further
determine that the primary design choice distinguishing VLA models lies in how
action tokens are formulated, which can be categorized into language
description, code, affordance, trajectory, goal state, latent representation,
raw action, and reasoning. However, there remains a lack of comprehensive
understanding regarding action tokens, significantly impeding effective VLA
development and obscuring future directions. Therefore, this survey aims to
categorize and interpret existing VLA research through the lens of action
tokenization, distill the strengths and limitations of each token type, and
identify areas for improvement. Through this systematic review and analysis, we
offer a synthesized outlook on the broader evolution of VLA models, highlight
underexplored yet promising directions, and contribute guidance for future
research, hoping to bring the field closer to general-purpose intelligence.",2025-07-02,2025,cs.RO,http://arxiv.org/pdf/2507.01925v1,2507.01925v1,arxiv,0,arXiv preprint
Decision-oriented Text Evaluation,"Yu-Shiang Huang, Chuan-Ju Wang, Chung-Chi Chen","Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.",2025-07-02,2025,cs.CL,http://arxiv.org/pdf/2507.01923v1,2507.01923v1,arxiv,0,arXiv preprint
Efficient stochastic simulation of gene regulatory networks using hybrid models of transcriptional bursting,"Mathilde Gaillard, Ulysse Herbach","Single-cell data reveal the presence of biological stochasticity between
cells of identical genome and environment, in particular highlighting the
transcriptional bursting phenomenon. To account for this property, gene
expression may be modeled as a continuous-time Markov chain where biochemical
species are described in a discrete way, leading to Gillespie's stochastic
simulation algorithm (SSA) which turns out to be computationally expensive for
realistic mRNA and protein copy numbers. Alternatively, hybrid models based on
piecewise-deterministic Markov processes (PDMPs) offer an effective compromise
for capturing cell-to-cell variability, but their simulation remains limited to
specialized mathematical communities. With a view to making them more
accessible, we present here a simple simulation method that is reminiscent of
SSA, while allowing for much lower computational cost. We detail the algorithm
for a bursty PDMP describing an arbitrary number of interacting genes, and
prove that it simulates exact trajectories of the model. As an illustration, we
use the algorithm to simulate a two-gene toggle switch: this example highlights
the fact that bimodal distributions as observed in real data are not explained
by transcriptional bursting per se, but rather by distinct burst frequencies
that may emerge from interactions between genes.",2025-07-02,2025,"q-bio.MN, math.PR",http://arxiv.org/pdf/2507.01922v1,2507.01922v1,arxiv,0,arXiv preprint
NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks,"Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li","Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality ""NaturalThoughts"" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.",2025-07-02,2025,cs.CL,http://arxiv.org/pdf/2507.01921v1,2507.01921v1,arxiv,0,arXiv preprint
Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models,"Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He","Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.",2025-07-02,2025,"cs.CL, cs.AI, cs.LG",http://arxiv.org/pdf/2507.01915v1,2507.01915v1,arxiv,0,arXiv preprint
On the influence of reference sample properties on magnetic force microscopy calibrations,"Baha Sakar, Christopher Habenschaden, Sibylle Sievers, Hans Werner Schumacher","Magnetic force microscopy (MFM) allows the characterization of magnetic stray
field distributions with high sensitivity and spatial resolution. Based on a
suitable calibration procedure, MFM can also yield quantitative magnetic field
values. This process typically involves measuring a reference sample to
determine the distribution of the tip's stray field or stray field gradient at
the sample surface. This distribution is called the tip transfer function (TTF)
and is derived through regularized deconvolution in Fourier space. The
properties of the reference sample and the noise characteristics of the
detection system significantly influence the derived TTF, thereby limiting its
validity range. In a recent study, the tip stray field distribution, and hence
the TTF, of an MFM tip was independently measured in real space using a
nitrogen vacancy center as a quantum sensor, revealing considerable
discrepancies with the reference-sample-based TTF. Here, we analyze the
influence of the feature distribution of the reference sample and the MFM
measurement parameters on the resulting TTF. We explain the observed
differences between quantum-calibrated stray field distributions and the
classical approach by attributing them to a loss of information due to missing
or suppressed spectral components. Furthermore, we emphasize the importance of
the spectral coverage of the TTF. Our findings indicate that for high-quality
reconstruction of the stray field of a sample under test (SUT), it is more
critical to ensure a strong overlap of frequency components between the
reference sample and the SUT than to achieve an accurate real-space
reconstruction of the tip stray field distribution.",2025-07-02,2025,cond-mat.mes-hall,http://arxiv.org/pdf/2507.01911v1,2507.01911v1,arxiv,0,arXiv preprint
Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning,"Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang","Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.",2025-07-02,2025,cs.CV,http://arxiv.org/pdf/2507.01908v1,2507.01908v1,arxiv,0,arXiv preprint
Attosecond Control and Measurement of Chiral Photoionisation Dynamics,"Meng Han, Jia-Bao Ji, Alexander Blech, R. Esteban Goetz, Corbin Allison, Loren Greenman, Christiane P. Koch, Hans Jakob Wörner","Many chirality-sensitive light-matter interactions are governed by chiral
electron dynamics. Therefore, the development of advanced technologies
harnessing chiral phenomena would critically benefit from measuring and
controlling chiral electron dynamics on their natural attosecond time scales.
Such endeavors have so far been hampered by the lack of characterized
circularly polarized attosecond pulses, an obstacle that has recently been
overcome (Han et al. Optica 10 (2023) 1044-1052, Han et al. Nature Physics 19
(2023) 230-236). In this article, we introduce chiroptical spectroscopy with
attosecond pulses and demonstrate attosecond coherent control over
photoelectron circular dichroism (PECD) (Goetz et al. Physical Review Letters
122 (2019) 013204, Goetz et al. arXiv:2104.07522), as well as the measurement
of chiral asymmetries in the forward-backward and angle-resolved
photoionisation delays of chiral molecules. We show that co-rotating attosecond
and near-infrared pulses can nearly double the PECD and even change its sign
compared to single-photon ionisation. We demonstrate that chiral
photoionisation delays depend on both polar and azimuthal angles of
photoemission in the light-propagation frame, requiring three-dimensional
momentum resolution. We measure forward-backward chiral-sensitive delays of up
to 120 as and polar-angle-resolved photoionisation delays up to 240 as, which
include an asymmmetry of $\sim$60 as originating from chirality in the
continuum-continuum transitions. Attosecond chiroptical spectroscopy opens the
door to quantitatively understanding and controlling the dynamics of chiral
molecules on the electronic time scale.",2025-07-02,2025,physics.chem-ph,http://arxiv.org/pdf/2507.01906v1,2507.01906v1,arxiv,0,arXiv preprint
AI4Research: A Survey of Artificial Intelligence for Scientific Research,"Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che","Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.",2025-07-02,2025,"cs.CL, cs.AI",http://arxiv.org/pdf/2507.01903v1,2507.01903v1,arxiv,0,arXiv preprint
High-Layer Attention Pruning with Rescaling,"Songtao Liu, Peng Liu","Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.",2025-07-02,2025,"cs.CL, cs.LG",http://arxiv.org/pdf/2507.01900v1,2507.01900v1,arxiv,0,arXiv preprint
The Frobenius number corresponding to the squares of three consecutive Fibonacci numbers: comparison of three algorithmic processes,"Aureliano M. Robles-Pérez, José Carlos Rosales","We compute the Frobenius number for numerical semigroups generated by the
squares of three consecutive Fibonacci numbers. We achieve this by using and
comparing three distinct algorithmic approaches: those developed by Ram\'irez
Alfons\'in and R{\o}dseth ([15]), Rosales and Garc\'ia-S\'anchez ([20]), and
Tripathi ([26]).",2025-07-02,2025,"math.NT, math.GR, 11D07, 20M13, 20M14",http://arxiv.org/pdf/2507.01898v1,2507.01898v1,arxiv,0,arXiv preprint
From Photospheric Footpoint Motion to Plasmoid Ejection: A Two-Stage Reconnection Process in a Small-scale Chromospheric Jet,"Zehao Tang, Yuandeng Shen, Chengrui Zhou, Surui Yao, Dongxu Liu, Xiaobo Li","Using high spatiotemporal resolution, multi-wavelength observations from the
New Vacuum Solar Telescope (NVST) and the Solar Dynamics Observatory (SDO), we
present a detailed analysis of a small-scale chromospheric jet driven by
plasmoid-mediated magnetic reconnection. Our results reveal that the entire
process is governed by the dynamic evolution of photospheric magnetic
footpoints, which proceeds in two distinct stages. An initial separating motion
of the footpoints corresponds to a mild reconnection phase, characterized by a
short current sheet and the eruption of a cool H$\alpha$ jet. Subsequently, a
converging motion of the footpoints triggers an intense reconnection phase.
During this intense stage, the current sheet rapidly elongates, and the
resulting decrease in its aspect ratio initiates a tearing-mode instability,
forming a plasmoid. The appearance of this plasmoid mediates the onset of fast
magnetic reconnection, which produces a hot EUV jet and is concurrent with
significant magnetic flux cancellation. We interpret this cancellation as the
submergence of newly formed, post-reconnection loops. Furthermore, we identify
a distinct, high-temperature plasma blob in the jet spire, significantly hotter
than the surrounding jet plasma. We attribute this feature to a secondary
heating process, likely caused by reconnection between the upward-propagating
plasmoid and the overlying magnetic cusp structure. These observations provide
a comprehensive, observationally driven picture (from the initial photospheric
triggers to the multi-stage, plasmoid-mediated reconnection) that forms
chromospheric jets, highlighting the critical role of footpoint motions in
solar atmospheric dynamics.",2025-07-02,2025,astro-ph.SR,http://arxiv.org/pdf/2507.01896v1,2507.01896v1,arxiv,0,arXiv preprint
$E_6$ Unification of Freeze-In Dark Matter and Leptogenesis,"Adeela Afzal, Rishav Roshan","We investigate the possibility of \emph{freeze-in} dark matter production and
baryogenesis in an $E_6$ extension of the Standard Model, featuring a residual
$U(1)_{\psi'}$ gauge symmetry. This symmetry arises from a linear combination
of $U(1)\chi$ and $U(1)_{\psi}$, both of which are subgroups of the $E_6$. The
spontaneous breaking of $U(1)_{\psi'}$ symmetry governs the dynamics of a
singlet fermion, which naturally serves as a freeze-in dark matter candidate.
The dark matter mass arises from dimension-five operators, and a discrete
symmetry ensures their stability. We show that freeze-in production from scalar
decay can yield the correct relic abundance for dark matter masses between few
MeV to a few hundred GeV. Simultaneously, heavy right-handed neutrinos generate
light neutrino masses via the type-I seesaw and produce the observed baryon
asymmetry via leptogenesis.",2025-07-02,2025,"hep-ph, astro-ph.CO, gr-qc, hep-th",http://arxiv.org/pdf/2507.01895v1,2507.01895v1,arxiv,0,arXiv preprint
Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders,"Nina R. Benway, Saba Tabatabaee, Dongliang Wang, Benjamin Munson, Jonathan L. Preston, Carol Espy-Wilson","Purpose: This study evaluated whether articulatory kinematics, inferred by
Articulatory Phonology speech inversion neural networks, aligned with
perceptual ratings of /r/ and /s/ in the speech of children with speech sound
disorders.
  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961
utterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual
ratings were standardized using the novel 5-point PERCEPT Rating Scale and
training protocol. Two research questions examined if the articulatory patterns
of inferred vocal tract variables aligned with the perceptual error category
for the phones investigated (e.g., tongue tip is more anterior in dentalized
/s/ productions than in correct /s/). A third research question examined if
gradient PERCEPT Rating Scale scores predicted articulatory proximity to
correct productions.
  Results: Estimated marginal means from linear mixed models supported 17 of 18
/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,
estimated marginal means from a second linear mixed model supported 7 of 15
hypotheses, particularly those related to the tongue tip. A third linear mixed
model revealed that PERCEPT Rating Scale scores significantly predicted
articulatory proximity of errored phones to correct productions.
  Conclusion: Inferred vocal tract variables differentiated category and
magnitude of articulatory errors for /r/, and to a lesser extent for /s/,
aligning with perceptual judgments. These findings support the clinical
interpretability of speech inversion vocal tract variables and the PERCEPT
Rating Scale in quantifying articulatory proximity to the target sound,
particularly for /r/.",2025-07-02,2025,eess.AS,http://arxiv.org/pdf/2507.01888v1,2507.01888v1,arxiv,0,arXiv preprint
MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants,"Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou","Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
""SLMs Learnability Gap"". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.",2025-07-02,2025,cs.CL,http://arxiv.org/pdf/2507.01887v1,2507.01887v1,arxiv,0,arXiv preprint
A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs,"Niccolò McConnell, Pardeep Vasudev, Daisuke Yamada, Daryl Cheng, Mehran Azimbagirad, John McCabe, Shahab Aslani, Ahmed H. Shahin, Yukun Zhou, The SUMMIT Consortium, Andre Altmann, Yipeng Hu, Paul Taylor, Sam M. Janes, Daniel C. Alexander, Joseph Jacob","Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.",2025-07-02,2025,"eess.IV, cs.CV, cs.LG",http://arxiv.org/pdf/2507.01881v1,2507.01881v1,arxiv,0,arXiv preprint
Joint Power Control and Precoding for Cell-Free Massive MIMO Systems With Sparse Multi-Dimensional Graph Neural Networks,"Yukun Ma, Jiayi Zhang, Ziheng Liu, Guowei Shi, Bo Ai","Cell-free massive multiple-input multiple-output (CF mMIMO) has emerged as a
prominent candidate for future networks due to its ability to significantly
enhance spectral efficiency by eliminating inter-cell interference. However,
its practical deployment faces considerable challenges, such as high
computational complexity and the optimization of its complex processing. To
address these challenges, this correspondence proposes a framework based on a
sparse multi-dimensional graph neural network (SP-MDGNN), which sparsifies the
connections between access points (APs) and user equipments (UEs) to
significantly reduce computational complexity while maintaining high
performance. In addition, the weighted minimum mean square error (WMMSE)
algorithm is introduced as a comparative method to further analyze the
trade-off between performance and complexity. Simulation results demonstrate
that the sparse method achieves an optimal balance between performance and
complexity, significantly reducing the computational complexity of the original
MDGNN method while incurring only a slight performance degradation, providing
insights for the practical deployment of CF mMIMO systems in large-scale
network.",2025-07-02,2025,"cs.IT, eess.SP, math.IT",http://arxiv.org/pdf/2507.01876v1,2507.01876v1,arxiv,0,arXiv preprint
