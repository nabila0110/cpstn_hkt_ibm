[
  {
    "title": "How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks",
    "authors": "Rahul Ramachandran, Ali Garjani, Roman Bachmann, Andrei Atanov, Oğuzhan Fatih Kar, Amir Zamir",
    "abstract": "Multimodal foundation models, such as GPT-4o, have recently made remarkable\nprogress, but it is not clear where exactly these models stand in terms of\nunderstanding vision. In this paper, we benchmark the performance of popular\nmultimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0\nFlash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision\ntasks (semantic segmentation, object detection, image classification, depth and\nsurface normal prediction) using established datasets (e.g., COCO, ImageNet and\nits variants, etc).\n  The main challenges to performing this are: 1) most models are trained to\noutput text and cannot natively express versatile domains, such as segments or\n3D geometry, and 2) many leading models are proprietary and accessible only at\nan API level, i.e., there is no weight access to adapt them. We address these\nchallenges by translating standard vision tasks into equivalent text-promptable\nand API-compatible tasks via prompt chaining to create a standardized\nbenchmarking framework.\n  We observe that 1) the models are not close to the state-of-the-art\nspecialist models at any task. However, 2) they are respectable generalists;\nthis is remarkable as they are presumably trained on primarily image-text-based\ntasks. 3) They perform semantic tasks notably better than geometric ones. 4)\nWhile the prompt-chaining techniques affect performance, better models exhibit\nless sensitivity to prompt variations. 5) GPT-4o performs the best among\nnon-reasoning models, securing the top position in 4 out of 6 tasks, 6)\nreasoning models, e.g. o3, show improvements in geometric tasks, and 7) a\npreliminary analysis of models with native image generation, like the latest\nGPT-4o, shows they exhibit quirks like hallucinations and spatial\nmisalignments.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV, cs.AI, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01955v1",
    "arxiv_id": "2507.01955v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Test-Time Scaling with Reflective Generative Model",
    "authors": "Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie",
    "abstract": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.LG, cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01951v1",
    "arxiv_id": "2507.01951v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Kwai Keye-VL Technical Report",
    "authors": "Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang",
    "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01949v1",
    "arxiv_id": "2507.01949v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Deep BSVIEs Parametrization and Learning-Based Applications",
    "authors": "Nacira Agram, Giulia Pucci",
    "abstract": "We study the numerical approximation of backward stochastic Volterra integral\nequations (BSVIEs) and their reflected extensions, which naturally arise in\nproblems with time inconsistency, path dependent preferences, and recursive\nutilities with memory. These equations generalize classical BSDEs by involving\ntwo dimensional time structures and more intricate dependencies.\n  We begin by developing a well posedness and measurability framework for\nBSVIEs in product probability spaces. Our approach relies on a representation\nof the solution as a parametrized family of backward stochastic equations\nindexed by the initial time, and draws on results of Stricker and Yor to ensure\nthat the two parameter solution is well defined in a joint measurable sense.\n  We then introduce a discrete time learning scheme based on a recursive\nbackward representation of the BSVIE, combining the discretization of Hamaguchi\nand Taguchi with deep neural networks. A detailed convergence analysis is\nprovided, generalizing the framework of deep BSDE solvers to the two\ndimensional BSVIE setting. Finally, we extend the solver to reflected BSVIEs,\nmotivated by applications in delayed recursive utility with lower constraints.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "math.PR",
    "pdf_url": "http://arxiv.org/pdf/2507.01948v1",
    "arxiv_id": "2507.01948v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Characterizing control between interacting subsystems with deep Jacobian estimation",
    "authors": "Adam J. Eisen, Mitchell Ostrow, Sarthak Chandra, Leo Kozachkov, Earl K. Miller, Ila R. Fiete",
    "abstract": "Biological function arises through the dynamical interactions of multiple\nsubsystems, including those between brain areas, within gene regulatory\nnetworks, and more. A common approach to understanding these systems is to\nmodel the dynamics of each subsystem and characterize communication between\nthem. An alternative approach is through the lens of control theory: how the\nsubsystems control one another. This approach involves inferring the\ndirectionality, strength, and contextual modulation of control between\nsubsystems. However, methods for understanding subsystem control are typically\nlinear and cannot adequately describe the rich contextual effects enabled by\nnonlinear complex systems. To bridge this gap, we devise a data-driven\nnonlinear control-theoretic framework to characterize subsystem interactions\nvia the Jacobian of the dynamics. We address the challenge of learning\nJacobians from time-series data by proposing the JacobianODE, a deep learning\nmethod that leverages properties of the Jacobian to directly estimate it for\narbitrary dynamical systems from data alone. We show that JacobianODEs\noutperform existing Jacobian estimation methods on challenging systems,\nincluding high-dimensional chaos. Applying our approach to a multi-area\nrecurrent neural network (RNN) trained on a working memory selection task, we\nshow that the \"sensory\" area gains greater control over the \"cognitive\" area\nover learning. Furthermore, we leverage the JacobianODE to directly control the\ntrained RNN, enabling precise manipulation of its behavior. Our work lays the\nfoundation for a theoretically grounded and data-driven understanding of\ninteractions among biological subsystems.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "q-bio.QM, cs.LG, math.DS, q-bio.NC",
    "pdf_url": "http://arxiv.org/pdf/2507.01946v1",
    "arxiv_id": "2507.01946v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "ML-Driven Strong Lens Discoveries: Down to $θ_E \\sim 0.03''$ and $M_\\mathrm{halo}< 10^{11} M_\\odot$",
    "authors": "Ethan Silver, R. Wang, Xiaosheng Huang, A. Bolton, C. Storfer, S. Banka",
    "abstract": "We present results on extending the strong lens discovery space down to much\nsmaller Einstein radii ($\\theta_E\\lesssim0.03''$) and much lower halo mass\n($M_\\mathrm{halo}<10^{11}M_\\odot$) through the combination of JWST observations\nand machine learning (ML) techniques. First, we forecast detectable strong\nlenses with JWST using CosmoDC2 as the lens catalog, and a source catalog down\nto 29th magnitude. By further incorporating the VELA hydrodynamical simulations\nof high-redshift galaxies, we simulate strong lenses. We train a ResNet on\nthese images, achieving near-100\\% completeness and purity for ``conventional\"\nstrong lenses ($\\theta_E\\gtrsim 0.5''$), applicable to JWST, HST, the Roman\nSpace Telescope and Euclid VIS. For the first time, we also search for very low\nhalo mass strong lenses ($M_{halo}<10^{11}M_\\odot$) in simulations, with\n$\\theta_E\\ll 0.5''$, down to the best resolution ($0.03''$) and depth\n(10,000~sec) limits of JWST using ResNet. A U-Net model is employed to pinpoint\nthese small lenses in images, which are otherwise virtually impossible for\nhuman detection. Our results indicate that JWST can find $\\sim 17$/deg$^2$ such\nlow-halo-mass lenses, with the locations of $\\sim 1.1$/deg$^2$ of these\ndetectable by the U-Net at $\\sim100$\\% precision (and $\\sim 7.0$/deg$^2$ at a\n99.0\\% precision). To validate our model for finding ``conventional\" strong\nlenses, we apply it to HST images, discovering two new strong lens candidates\npreviously missed by human classifiers in a crowdsourcing project (Garvin et\nal. 2022). This study demonstrates the (potentially ``superhuman\") advantages\nof ML combined with current and future space telescopes for detecting\nconventional, and especially, low-halo-mass strong lenses, which are critical\nfor testing CDM models.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "astro-ph.CO, astro-ph.GA",
    "pdf_url": "http://arxiv.org/pdf/2507.01943v1",
    "arxiv_id": "2507.01943v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
    "authors": "Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo",
    "abstract": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "astro-ph.IM, astro-ph.SR, cs.AI, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01939v1",
    "arxiv_id": "2507.01939v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition",
    "authors": "Zhaosong Lu, Xiangyuan Wang",
    "abstract": "We study a class of nonconvex-nonconcave minimax problems in which the inner\nmaximization problem satisfies a local Kurdyka-{\\L}ojasiewicz (KL) condition\nthat may vary with the outer minimization variable. In contrast to the global\nKL or Polyak-{\\L}ojasiewicz (PL) conditions commonly assumed in the literature\n-- which are significantly stronger and often too restrictive in practice --\nthis local KL condition accommodates a broader range of practical scenarios.\nHowever, it also introduces new analytical challenges. In particular, as an\noptimization algorithm progresses toward a stationary point of the problem, the\nregion over which the KL condition holds may shrink, resulting in a more\nintricate and potentially ill-conditioned landscape. To address this challenge,\nwe show that the associated maximal function is locally H\\\"older smooth.\nLeveraging this key property, we develop an inexact proximal gradient method\nfor solving the minimax problem, where the inexact gradient of the maximal\nfunction is computed by applying a proximal gradient method to a KL-structured\nsubproblem. Under mild assumptions, we establish complexity guarantees for\ncomputing an approximate stationary point of the minimax problem.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "math.OC, cs.LG, cs.NA, math.NA, stat.ML, 90C26, 90C30, 90C47, 90C99, 65K05",
    "pdf_url": "http://arxiv.org/pdf/2507.01932v1",
    "arxiv_id": "2507.01932v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla",
    "authors": "Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman",
    "abstract": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.AI, cs.SD, eess.AS",
    "pdf_url": "http://arxiv.org/pdf/2507.01931v1",
    "arxiv_id": "2507.01931v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "IC-Custom: Diverse Image Customization via In-Context Learning",
    "authors": "Yaowei Li, Xiaoyu Li, Zhaoyang Zhang, Yuxuan Bian, Gan Liu, Xinyuan Li, Jiale Xu, Wenbo Hu, Yating Liu, Lingen Li, Jing Cai, Yuexian Zou, Yancheng He, Ying Shan",
    "abstract": "Image customization, a crucial technique for industrial media production,\naims to generate content that is consistent with reference images. However,\ncurrent approaches conventionally separate image customization into\nposition-aware and position-free customization paradigms and lack a universal\nframework for diverse customization, limiting their applications across various\nscenarios. To overcome these limitations, we propose IC-Custom, a unified\nframework that seamlessly integrates position-aware and position-free image\ncustomization through in-context learning. IC-Custom concatenates reference\nimages with target images to a polyptych, leveraging DiT's multi-modal\nattention mechanism for fine-grained token-level interactions. We introduce the\nIn-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented\nregister tokens and boundary-aware positional embeddings to enable the model to\ncorrectly handle different task types and distinguish various inputs in\npolyptych configurations. To bridge the data gap, we carefully curated a\nhigh-quality dataset of 12k identity-consistent samples with 8k from real-world\nsources and 4k from high-quality synthetic data, avoiding the overly glossy and\nover-saturated synthetic appearance. IC-Custom supports various industrial\napplications, including try-on, accessory placement, furniture arrangement, and\ncreative IP customization. Extensive evaluations on our proposed ProductBench\nand the publicly available DreamBench demonstrate that IC-Custom significantly\noutperforms community workflows, closed-source models, and state-of-the-art\nopen-source approaches. IC-Custom achieves approximately 73% higher human\npreference across identity consistency, harmonicity, and text alignment\nmetrics, while training only 0.4% of the original model parameters. Project\npage: https://liyaowei-stu.github.io/project/IC_Custom",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01926v1",
    "arxiv_id": "2507.01926v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection",
    "authors": "Samirah Bakker, Yao Ma, Seyed Sahand Mohammadi Ziabari",
    "abstract": "The complexity of mental healthcare billing enables anomalies, including\nfraud. While machine learning methods have been applied to anomaly detection,\nthey often struggle with class imbalance, label scarcity, and complex\nsequential patterns. This study explores a hybrid deep learning approach\ncombining Long Short-Term Memory (LSTM) networks and Transformers, with\npseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior\nwork has not evaluated such hybrid models trained on pseudo-labeled data in the\ncontext of healthcare billing. The approach is evaluated on two real-world\nbilling datasets related to mental healthcare. The iForest LSTM baseline\nachieves the highest recall (0.963) on declaration-level data. On the\noperation-level data, the hybrid iForest-based model achieves the highest\nrecall (0.744), though at the cost of lower precision. These findings highlight\nthe potential of combining pseudo-labeling with hybrid deep learning in\ncomplex, imbalanced anomaly detection settings.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.LG, cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.01924v1",
    "arxiv_id": "2507.01924v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks",
    "authors": "Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li",
    "abstract": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01921v1",
    "arxiv_id": "2507.01921v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning",
    "authors": "Christian Bongiorno, Efstratios Manolakis, Rosario Nunzio Mantegna",
    "abstract": "We develop a rotation-invariant neural network that provides the global\nminimum-variance portfolio by jointly learning how to lag-transform historical\nreturns and how to regularise both the eigenvalues and the marginal\nvolatilities of large equity covariance matrices. This explicit mathematical\nmapping offers clear interpretability of each module's role, so the model\ncannot be regarded as a pure black-box. The architecture mirrors the analytical\nform of the global minimum-variance solution yet remains agnostic to dimension,\nso a single model can be calibrated on panels of a few hundred stocks and\napplied, without retraining, to one thousand US equities-a cross-sectional jump\nthat demonstrates robust out-of-sample generalisation. The loss function is the\nfuture realized minimum portfolio variance and is optimized end-to-end on real\ndaily returns. In out-of-sample tests from January 2000 to December 2024 the\nestimator delivers systematically lower realised volatility, smaller maximum\ndrawdowns, and higher Sharpe ratios than the best analytical competitors,\nincluding state-of-the-art non-linear shrinkage. Furthermore, although the\nmodel is trained end-to-end to produce an unconstrained (long-short)\nminimum-variance portfolio, we show that its learned covariance representation\ncan be used in general optimizers under long-only constraints with virtually no\nloss in its performance advantage over competing estimators. These gains\npersist when the strategy is executed under a highly realistic implementation\nframework that models market orders at the auctions, empirical slippage,\nexchange fees, and financing charges for leverage, and they remain stable\nduring episodes of acute market stress.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "q-fin.PM, cs.AI, math.OC, physics.data-an, stat.ML, 91G10 (Primary) 68T07, 91G60, 62P05 (Secondary), I.2.6; I.5.1; G.3; J.4",
    "pdf_url": "http://arxiv.org/pdf/2507.01918v1",
    "arxiv_id": "2507.01918v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models",
    "authors": "Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.AI, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01915v1",
    "arxiv_id": "2507.01915v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction",
    "authors": "Apoorv Verma, Junaid Jami, Amrita Bhattacharya",
    "abstract": "Accurately predicting magnetic behavior across diverse materials systems\nremains a longstanding challenge due to the complex interplay of structural and\nelectronic factors and is pivotal for the accelerated discovery and design of\nnext-generation magnetic materials. In this work, a refined descriptor is\nproposed that significantly improves the prediction of two critical magnetic\nproperties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic\nmoment per atom -- using only the structural information of materials. Unlike\nprevious models limited to Mn-based or lanthanide-transition metal compounds,\nthe present approach generalizes across a diverse dataset of 5741 stable,\nbinary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the\nMaterials Project. Leveraging an enriched elemental vector representation and\nadvanced feature engineering, including nonlinear terms and reduced matrix\nsparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic\nordering classification and balanced recall across FM and FiM classes,\naddressing a key limitation in prior studies. The model predicts magnetic\nmoment per atom with a correlation coefficient of 0.93, surpassing the Hund's\nmatrix and orbital field matrix descriptors. Additionally, it accurately\nestimates formation energy per atom, enabling assessment of both magnetic\nbehavior and material stability. This generalized and computationally efficient\nframework offers a robust tool for high-throughput screening of magnetic\nmaterials with tailored properties.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cond-mat.mtrl-sci, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01913v1",
    "arxiv_id": "2507.01913v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP",
    "authors": "Ranjan Sapkota, Zhichao Meng, Martin Churuvija, Xiaoqiang Du, Zenghong Ma, Manoj Karkee",
    "abstract": "In orchard automation, dense foliage during the canopy season severely\noccludes tree structures, minimizing visibility to various canopy parts such as\ntrunks and branches, which limits the ability of a machine vision system.\nHowever, canopy structure is more open and visible during the dormant season\nwhen trees are defoliated. In this work, we present an information fusion\nframework that integrates multi-seasonal structural data to support robotic and\nautomated crop load management during the entire growing season. The framework\ncombines high-resolution RGB-D imagery from both dormant and canopy periods\nusing YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D\nreconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for\nmodel alignment. Segmentation outputs from YOLOv9-Seg were used to extract\ndepth-informed masks, which enabled accurate 3D point cloud reconstruction via\nKinect Fusion; these reconstructed models from each season were subsequently\naligned using Fast GICP to achieve spatially coherent multi-season fusion. The\nYOLOv9-Seg model, trained on manually annotated images, achieved a mean squared\nerror (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in\ndormant season dataset. Kinect Fusion enabled accurate reconstruction of tree\ngeometry, validated with field measurements resulting in root mean square\nerrors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and\n13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal\nregistration with a minimum fitness score of 0.00197, allowing integrated,\ncomprehensive tree structure modeling despite heavy occlusions during the\ngrowing season. This fused structural representation enables robotic systems to\naccess otherwise obscured architectural information, improving the precision of\npruning, thinning, and other automated orchard operations.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01912v1",
    "arxiv_id": "2507.01912v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "High-Layer Attention Pruning with Rescaling",
    "authors": "Songtao Liu, Peng Liu",
    "abstract": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01900v1",
    "arxiv_id": "2507.01900v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "STEM Diffraction Pattern Analysis with Deep Learning Networks",
    "authors": "Sebastian Wissel, Jonas Scheunert, Aaron Dextre, Shamail Ahmed, Andreas Bayer, Kerstin Volz, Bai-Xiang Xu",
    "abstract": "Accurate grain orientation mapping is essential for understanding and\noptimizing the performance of polycrystalline materials, particularly in\nenergy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising\ncathode material for next-generation lithium-ion batteries, and its\nelectrochemical behaviour is closely linked to microstructural features such as\ngrain size and crystallographic orientations. Traditional orientation mapping\nmethods--such as manual indexing, template matching (TM), or Hough\ntransform-based techniques--are often slow and noise-sensitive when handling\ncomplex or overlapping patterns, creating a bottleneck in large-scale\nmicrostructural analysis. This work presents a machine learning-based approach\nfor predicting Euler angles directly from scanning transmission electron\nmicroscopy (STEM) diffraction patterns (DPs). This enables the automated\ngeneration of high-resolution crystal orientation maps, facilitating the\nanalysis of internal microstructures at the nanoscale. Three deep learning\narchitectures--convolutional neural networks (CNNs), Dense Convolutional\nNetworks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,\nusing an experimentally acquired dataset labelled via a commercial TM\nalgorithm. While the CNN model serves as a baseline, both DenseNets and Swin\nTransformers demonstrate superior performance, with the Swin Transformer\nachieving the highest evaluation scores and the most consistent microstructural\npredictions. The resulting crystal maps exhibit clear grain boundary\ndelineation and coherent intra-grain orientation distributions, underscoring\nthe potential of attention-based architectures for analyzing diffraction-based\nimage data. These findings highlight the promise of combining advanced machine\nlearning models with STEM data for robust, high-throughput microstructural\ncharacterization.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cond-mat.dis-nn, cond-mat.mtrl-sci, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01889v1",
    "arxiv_id": "2507.01889v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants",
    "authors": "Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou",
    "abstract": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01887v1",
    "arxiv_id": "2507.01887v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Improving GANs by leveraging the quantum noise from real hardware",
    "authors": "Hongni Jin, Kenneth M. Merz Jr",
    "abstract": "We propose a novel approach to generative adversarial networks (GANs) in\nwhich the standard i.i.d. Gaussian latent prior is replaced or hybridized with\na quantum-correlated prior derived from measurements of a 16-qubit entangling\ncircuit. Each latent sample is generated by grouping repeated shots per qubit\ninto a binary fraction, applying the inverse Gaussian CDF to obtain a\n16-dimensional Gaussian vector whose joint copula reflects genuine quantum\nentanglement, and then projecting into the high-dimensional space via a fixed\nrandom matrix. By pre-sampling tens of millions of bitstrings, either from a\nnoiseless simulator or from IBM hardware, we build large pools of independent\nbut internally quantum-correlated latents. We integrate this prior into three\nrepresentative architectures (WGAN, SNGAN, BigGAN) on CIFAR-10, making no\nchanges to the neural network structure or training hyperparameters. The hybrid\nlatent representations incorporating hardware-derived noise consistently lower\nthe FID relative to both the classical baseline and the simulator variant,\nespecially when the quantum component constitutes a substantial fraction of the\nprior. In addition, we execute on the QPU in parallel to not only save\ncomputing time but also further decrease the FID up to 17% in BigGAN. These\nresults indicate that intrinsic quantum randomness and device-specific\nimperfections can provide a structured inductive bias that enhances GAN\nperformance. Our work demonstrates a practical pipeline for leveraging noisy\nquantum hardware to enrich deep-generative modeling, opening a new interface\nbetween quantum information and machine learning. All code and data are\navailable at https://github.com/Neon8988/GAN_QN.git.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "quant-ph",
    "pdf_url": "http://arxiv.org/pdf/2507.01886v1",
    "arxiv_id": "2507.01886v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification",
    "authors": "Kunlun Xu, Fan Zhuo, Jiangmeng Li, Xu Zou, Jiahuan Zhou",
    "abstract": "Current lifelong person re-identification (LReID) methods predominantly rely\non fully labeled data streams. However, in real-world scenarios where\nannotation resources are limited, a vast amount of unlabeled data coexists with\nscarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)\nproblem where LReID methods suffer severe performance degradation. Existing\nLReID methods, even when combined with semi-supervised strategies, suffer from\nlimited long-term adaptation performance due to struggling with the noisy\nknowledge occurring during unlabeled data utilization. In this paper, we\npioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing\nPrototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key\ninnovation lies in establishing a self-reinforcing cycle between dynamic\nprototype-guided pseudo-label generation and new-old knowledge collaborative\npurification to enhance the utilization of unlabeled data. Specifically,\nlearnable identity prototypes are introduced to dynamically capture the\nidentity distributions and generate high-quality pseudo-labels. Then, the\ndual-knowledge cooperation scheme integrates current model specialization and\nhistorical model generalization, refining noisy pseudo-labels. Through this\ncyclic design, reliable pseudo-labels are progressively mined to improve\ncurrent-stage learning and ensure positive knowledge propagation over long-term\nlearning. Experiments on the established Semi-LReID benchmarks show that our\nSPRED achieves state-of-the-art performance. Our source code is available at\nhttps://github.com/zhoujiahuan1991/ICCV2025-SPRED",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01884v1",
    "arxiv_id": "2507.01884v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Future Slot Prediction for Unsupervised Object Discovery in Surgical Video",
    "authors": "Guiqiu Liao, Matjaz Jogan, Marcel Hussing, Edward Zhang, Eric Eaton, Daniel A. Hashimoto",
    "abstract": "Object-centric slot attention is an emerging paradigm for unsupervised\nlearning of structured, interpretable object-centric representations (slots).\nThis enables effective reasoning about objects and events at a low\ncomputational cost and is thus applicable to critical healthcare applications,\nsuch as real-time interpretation of surgical video. The heterogeneous scenes in\nreal-world applications like surgery are, however, difficult to parse into a\nmeaningful set of slots. Current approaches with an adaptive slot count perform\nwell on images, but their performance on surgical videos is low. To address\nthis challenge, we propose a dynamic temporal slot transformer (DTST) module\nthat is trained both for temporal reasoning and for predicting the optimal\nfuture slot initialization. The model achieves state-of-the-art performance on\nmultiple surgical databases, demonstrating that unsupervised object-centric\nmethods can be applied to real-world data and become part of the common arsenal\nin healthcare applications.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01882v1",
    "arxiv_id": "2507.01882v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs",
    "authors": "Niccolò McConnell, Pardeep Vasudev, Daisuke Yamada, Daryl Cheng, Mehran Azimbagirad, John McCabe, Shahab Aslani, Ahmed H. Shahin, Yukun Zhou, The SUMMIT Consortium, Andre Altmann, Yipeng Hu, Paul Taylor, Sam M. Janes, Daniel C. Alexander, Joseph Jacob",
    "abstract": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening\n(LCS) programs is increasing in uptake worldwide. LCS programs herald a\ngenerational opportunity to simultaneously detect cancer and non-cancer-related\nearly-stage lung disease. Yet these efforts are hampered by a shortage of\nradiologists to interpret scans at scale. Here, we present TANGERINE, a\ncomputationally frugal, open-source vision foundation model for volumetric LDCT\nanalysis. Designed for broad accessibility and rapid adaptation, TANGERINE can\nbe fine-tuned off the shelf for a wide range of disease-specific tasks with\nlimited computational resources and training data. Relative to models trained\nfrom scratch, TANGERINE demonstrates fast convergence during fine-tuning,\nthereby requiring significantly fewer GPU hours, and displays strong label\nefficiency, achieving comparable or superior performance with a fraction of\nfine-tuning data. Pretrained using self-supervised learning on over 98,000\nthoracic LDCTs, including the UK's largest LCS initiative to date and 27 public\ndatasets, TANGERINE achieves state-of-the-art performance across 14 disease\nclassification tasks, including lung cancer and multiple respiratory diseases,\nwhile generalising robustly across diverse clinical centres. By extending a\nmasked autoencoder framework to 3D imaging, TANGERINE offers a scalable\nsolution for LDCT analysis, departing from recent closed, resource-intensive\nmodels by combining architectural simplicity, public availability, and modest\ncomputational requirements. Its accessible, open-source lightweight design lays\nthe foundation for rapid integration into next-generation medical imaging tools\nthat could transform LCS initiatives, allowing them to pivot from a singular\nfocus on lung cancer detection to comprehensive respiratory disease management\nin high-risk populations.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "eess.IV, cs.CV, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01881v1",
    "arxiv_id": "2507.01881v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Evolving HPC services to enable ML workloads on HPE Cray EX",
    "authors": "Stefano Schuppli, Fawzi Mohamed, Henrique Mendonça, Nina Mujkanovic, Elia Palme, Dino Conciatore, Lukas Drescher, Miguel Gila, Pim Witlox, Joost VandeVondele, Maxime Martinasso, Thomas C. Schulthess, Torsten Hoefler",
    "abstract": "The Alps Research Infrastructure leverages GH200 technology at scale,\nfeaturing 10,752 GPUs. Accessing Alps provides a significant computational\nadvantage for researchers in Artificial Intelligence (AI) and Machine Learning\n(ML). While Alps serves a broad range of scientific communities, traditional\nHPC services alone are not sufficient to meet the dynamic needs of the ML\ncommunity. This paper presents an initial investigation into extending HPC\nservice capabilities to better support ML workloads. We identify key challenges\nand gaps we have observed since the early-access phase (2023) of Alps by the\nSwiss AI community and propose several technological enhancements. These\ninclude a user environment designed to facilitate the adoption of HPC for ML\nworkloads, balancing performance with flexibility; a utility for rapid\nperformance screening of ML applications during development; observability\ncapabilities and data products for inspecting ongoing large-scale ML workloads;\na utility to simplify the vetting of allocated nodes for compute readiness; a\nservice plane infrastructure to deploy various types of workloads, including\nsupport and inference services; and a storage infrastructure tailored to the\nspecific needs of ML workloads. These enhancements aim to facilitate the\nexecution of ML workloads on HPC systems, increase system usability and\nresilience, and better align with the needs of the ML community. We also\ndiscuss our current approach to security aspects. This paper concludes by\nplacing these proposals in the broader context of changes in the communities\nserved by HPC infrastructure like ours.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.DC, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01880v1",
    "arxiv_id": "2507.01880v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Towards Foundation Auto-Encoders for Time-Series Anomaly Detection",
    "authors": "Gastón García González, Pedro Casas, Emilio Martínez, Alicia Fernández",
    "abstract": "We investigate a novel approach to time-series modeling, inspired by the\nsuccesses of large pretrained foundation models. We introduce FAE (Foundation\nAuto-Encoders), a foundation generative-AI model for anomaly detection in\ntime-series data, based on Variational Auto-Encoders (VAEs). By foundation, we\nmean a model pretrained on massive amounts of time-series data which can learn\ncomplex temporal patterns useful for accurate modeling, forecasting, and\ndetection of anomalies on previously unseen datasets. FAE leverages VAEs and\nDilated Convolutional Neural Networks (DCNNs) to build a generic model for\nunivariate time-series modeling, which could eventually perform properly in\nout-of-the-box, zero-shot anomaly detection applications. We introduce the main\nconcepts of FAE, and present preliminary results in different multi-dimensional\ntime-series datasets from various domains, including a real dataset from an\noperational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.LG, cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.01875v1",
    "arxiv_id": "2507.01875v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
    "authors": "Kenan Tang, Yanhong Li, Yao Qin",
    "abstract": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01872v1",
    "arxiv_id": "2507.01872v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Breaking the $n^{1.5}$ Additive Error Barrier for Private and Efficient Graph Sparsification via Private Expander Decomposition",
    "authors": "Anders Aamand, Justin Y. Chen, Mina Dalirrooyfard, Slobodan Mitrović, Yuriy Nevmyvaka, Sandeep Silwal, Yinzhan Xu",
    "abstract": "We study differentially private algorithms for graph cut sparsification, a\nfundamental problem in algorithms, privacy, and machine learning. While\nsignificant progress has been made, the best-known private and efficient cut\nsparsifiers on $n$-node graphs approximate each cut within\n$\\widetilde{O}(n^{1.5})$ additive error and $1+\\gamma$ multiplicative error for\nany $\\gamma > 0$ [Gupta, Roth, Ullman TCC'12]. In contrast, \"inefficient\"\nalgorithms, i.e., those requiring exponential time, can achieve an\n$\\widetilde{O}(n)$ additive error and $1+\\gamma$ multiplicative error\n[Eli{\\'a}{\\v{s}}, Kapralov, Kulkarni, Lee SODA'20]. In this work, we break the\n$n^{1.5}$ additive error barrier for private and efficient cut sparsification.\nWe present an $(\\varepsilon,\\delta)$-DP polynomial time algorithm that, given a\nnon-negative weighted graph, outputs a private synthetic graph approximating\nall cuts with multiplicative error $1+\\gamma$ and additive error $n^{1.25 +\no(1)}$ (ignoring dependencies on $\\varepsilon, \\delta, \\gamma$).\n  At the heart of our approach lies a private algorithm for expander\ndecomposition, a popular and powerful technique in (non-private) graph\nalgorithms.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.DS",
    "pdf_url": "http://arxiv.org/pdf/2507.01873v1",
    "arxiv_id": "2507.01873v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Direct Vertex Reconstruction of $Λ$ Baryons from Hits in CLAS12 using Graph Neural Networks",
    "authors": "Keegan Menkce, Matthew McEneaney, Anselm Vossen",
    "abstract": "Machine learning techniques, including Graph Neural Networks (GNNs), have\nbeen used extensively for data analysis in high energy and nuclear physics.\nHere we report on the use of a GNN to reconstruct decay vertices of $\\Lambda$\nhyperons directly from hits in the tracking detector at the CLAS12 experiment\nat Jefferson Laboratory (JLab). We show that we can improve the vertex\nreconstruction in simulation compared to the standard, track based, algorithm.\nWe believe this warrants further study. The current study is limited by\navailable training resources but points to an interesting possibility to forgo\nvertex reconstruction by track fitting in a complicated magnetic field for a\nmore direct approach where the hit to vertex mapping is encoded in a neural\nnetwork.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "hep-ex",
    "pdf_url": "http://arxiv.org/pdf/2507.01868v1",
    "arxiv_id": "2507.01868v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "An in-silico lung phantom to assess the performance of pulmonary artery segmentation using angiogram",
    "authors": "Sunder Neelakantan, Tanmay Mukherjee, Emilio A. Mendiola, Kyle Myers, Reza Avazmohammadi",
    "abstract": "Pulmonary hypertension (PH) can lead to significant vascular remodeling,\nresulting in altered pulmonary blood flow. Estimating the patient-specific\ncontributions of each remodeling event is necessary to optimize and\nindividualize clinical intervention strategies. In-silico modeling has emerged\nas a powerful tool to simulate pulmonary hemodynamics, and one of the primary\nrequirements for robust in-silico modeling is an accurate representation of the\npulmonary vasculature structure. Computed tomography (CT) imaging can be used\nto segment and reconstruct the proximal vasculature. However, contrast-enhanced\nimaging, such as CT pulmonary angiography, is required to obtain a\ncomprehensive and high-fidelity view of the pulmonary vasculature. The clinical\nuse of CT pulmonary angiography is limited by the complications associated with\nthe injection of contrast agents. Machine learning (ML) approaches have emerged\nto effectively segment and reconstruct the pulmonary vasculature without the\nneed for contrast-enhanced imaging. We have developed a method to create\nin-silico pulmonary angiogram phantoms with varying simulated contrast levels.\nThe results indicated that adding simulated contrast can allow for successful\nsegmentation of the pulmonary vasculature. We expect this method to assist with\ndeveloping and training ML-based segmentation frameworks and aid in their\nvalidation, thereby improving the capability to segment and reconstruct\npulmonary vasculature without using contrast-enhanced imaging.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "physics.bio-ph",
    "pdf_url": "http://arxiv.org/pdf/2507.01867v1",
    "arxiv_id": "2507.01867v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types",
    "authors": "Yuhao Lin, Yi-Lin Wei, Haoran Liao, Mu Lin, Chengyi Xing, Hao Li, Dandan Zhang, Mark Cutkosky, Wei-Shi Zheng",
    "abstract": "Dexterous teleoperation plays a crucial role in robotic manipulation for\nreal-world data collection and remote robot control. Previous dexterous\nteleoperation mostly relies on hand retargeting to closely mimic human hand\npostures. However, these approaches may fail to fully leverage the inherent\ndexterity of dexterous hands, which can execute unique actions through their\nstructural advantages compared to human hands. To address this limitation, we\npropose TypeTele, a type-guided dexterous teleoperation system, which enables\ndexterous hands to perform actions that are not constrained by human motion\npatterns. This is achieved by introducing dexterous manipulation types into the\nteleoperation system, allowing operators to employ appropriate types to\ncomplete specific tasks. To support this system, we build an extensible\ndexterous manipulation type library to cover comprehensive dexterous postures\nused in manipulation tasks. During teleoperation, we employ a MLLM\n(Multi-modality Large Language Model)-assisted type retrieval module to\nidentify the most suitable manipulation type based on the specific task and\noperator commands. Extensive experiments of real-world teleoperation and\nimitation learning demonstrate that the incorporation of manipulation types\nsignificantly takes full advantage of the dexterous robot's ability to perform\ndiverse and complex tasks with higher success rates.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2507.01857v1",
    "arxiv_id": "2507.01857v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation",
    "authors": "Sixiang Chen, Jiaming Liu, Siyuan Qian, Han Jiang, Lily Li, Renrui Zhang, Zhuoyang Liu, Chenyang Gu, Chengkai Hou, Pengwei Wang, Zhongyuan Wang, Shanghang Zhang",
    "abstract": "Recently, mobile manipulation has attracted increasing attention for enabling\nlanguage-conditioned robotic control in household tasks. However, existing\nmethods still face challenges in coordinating mobile base and manipulator,\nprimarily due to two limitations. On the one hand, they fail to explicitly\nmodel the influence of the mobile base on manipulator control, which easily\nleads to error accumulation under high degrees of freedom. On the other hand,\nthey treat the entire mobile manipulation process with the same visual\nobservation modality (e.g., either all 2D or all 3D), overlooking the distinct\nmultimodal perception requirements at different stages during mobile\nmanipulation. To address this, we propose the Adaptive Coordination Diffusion\nTransformer (AC-DiT), which enhances mobile base and manipulator coordination\nfor end-to-end mobile manipulation. First, since the motion of the mobile base\ndirectly influences the manipulator's actions, we introduce a mobility-to-body\nconditioning mechanism that guides the model to first extract base motion\nrepresentations, which are then used as context prior for predicting whole-body\nactions. This enables whole-body control that accounts for the potential impact\nof the mobile base's motion. Second, to meet the perception requirements at\ndifferent stages of mobile manipulation, we design a perception-aware\nmultimodal conditioning strategy that dynamically adjusts the fusion weights\nbetween various 2D visual images and 3D point clouds, yielding visual features\ntailored to the current perceptual needs. This allows the model to, for\nexample, adaptively rely more on 2D inputs when semantic information is crucial\nfor action prediction, while placing greater emphasis on 3D geometric\ninformation when precise spatial understanding is required. We validate AC-DiT\nthrough extensive experiments on both simulated and real-world mobile\nmanipulation tasks.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.RO, cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.01961v1",
    "arxiv_id": "2507.01961v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation",
    "authors": "Zhuoyang Zhang, Luke J. Huang, Chengyue Wu, Shang Yang, Kelly Peng, Yao Lu, Song Han",
    "abstract": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256$\\times$256 res.) and 1024 to 48 (512$\\times$512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4$\\times$ lower latency than previous parallelized\nautoregressive models.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV, cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.01957v1",
    "arxiv_id": "2507.01957v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
    "authors": "Yukang Cao, Chenyang Si, Jinghao Wang, Ziwei Liu",
    "abstract": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01953v1",
    "arxiv_id": "2507.01953v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Test-Time Scaling with Reflective Generative Model",
    "authors": "Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie",
    "abstract": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.LG, cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01951v1",
    "arxiv_id": "2507.01951v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "String Breaking Dynamics and Glueball Formation in a $2+1$D Lattice Gauge Theory",
    "authors": "Kaidi Xu, Umberto Borla, Sergej Moroz, Jad C. Halimeh",
    "abstract": "With the advent of advanced quantum processors capable of probing lattice\ngauge theories (LGTs) in higher spatial dimensions, it is crucial to understand\nstring dynamics in such models to guide upcoming experiments and to make\nconnections to high-energy physics (HEP). Using tensor network methods, we\nstudy the far-from-equilibrium quench dynamics of electric flux strings between\ntwo static charges in the $2+1$D $\\mathbb{Z}_2$ LGT with dynamical matter. We\ncalculate the probabilities of finding the time-evolved wave function in string\nconfigurations of the same length as the initial string. At resonances\ndetermined by the the electric field strength and the mass, we identify various\nstring breaking processes accompanied with matter creation. Away from resonance\nstrings exhibit intriguing confined dynamics which, for strong electric fields,\nwe fully characterize through effective perturbative models. Starting in\nmaximal-length strings, we find that the wave function enters a dynamical\nregime where it splits into shorter strings and disconnected loops, with the\nlatter bearing qualitative resemblance to glueballs in quantum chromodynamics\n(QCD). Our findings can be probed on state-of-the-art superconducting-qubit and\ntrapped-ion quantum processors.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "hep-lat, cond-mat.quant-gas, cond-mat.stat-mech, hep-th, quant-ph",
    "pdf_url": "http://arxiv.org/pdf/2507.01950v1",
    "arxiv_id": "2507.01950v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Kwai Keye-VL Technical Report",
    "authors": "Kwai Keye Team, Biao Yang, Bin Wen, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Hao Peng, Haojie Ding, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Jin Ouyang, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yang Zhou, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zhenhua Wu, Zhenyu Li, Zhixin Ling, Ziming Li, Dehua Ma, Di Xu, Haixuan Gao, Hang Li, Jiawei Guo, Jing Wang, Lejian Ren, Muhao Wei, Qianqian Wang, Qigen Hu, Shiyao Wang, Tao Yu, Xinchen Luo, Yan Li, Yiming Liang, Yuhang Hu, Zeyi Lu, Zhuoran Yang, Zixing Zhang",
    "abstract": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce \\textbf{Kwai Keye-VL}, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the \\textbf{KC-MMBench}, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01949v1",
    "arxiv_id": "2507.01949v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Deep BSVIEs Parametrization and Learning-Based Applications",
    "authors": "Nacira Agram, Giulia Pucci",
    "abstract": "We study the numerical approximation of backward stochastic Volterra integral\nequations (BSVIEs) and their reflected extensions, which naturally arise in\nproblems with time inconsistency, path dependent preferences, and recursive\nutilities with memory. These equations generalize classical BSDEs by involving\ntwo dimensional time structures and more intricate dependencies.\n  We begin by developing a well posedness and measurability framework for\nBSVIEs in product probability spaces. Our approach relies on a representation\nof the solution as a parametrized family of backward stochastic equations\nindexed by the initial time, and draws on results of Stricker and Yor to ensure\nthat the two parameter solution is well defined in a joint measurable sense.\n  We then introduce a discrete time learning scheme based on a recursive\nbackward representation of the BSVIE, combining the discretization of Hamaguchi\nand Taguchi with deep neural networks. A detailed convergence analysis is\nprovided, generalizing the framework of deep BSDE solvers to the two\ndimensional BSVIE setting. Finally, we extend the solver to reflected BSVIEs,\nmotivated by applications in delayed recursive utility with lower constraints.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "math.PR",
    "pdf_url": "http://arxiv.org/pdf/2507.01948v1",
    "arxiv_id": "2507.01948v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
    "authors": "Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo",
    "abstract": "In recent years, large language models (LLMs) have transformed natural\nlanguage understanding through vast datasets and large-scale parameterization.\nInspired by this success, we present SpecCLIP, a foundation model framework\nthat extends LLM-inspired methodologies to stellar spectral analysis. Stellar\nspectra, akin to structured language, encode rich physical and chemical\ninformation about stars. By training foundation models on large-scale spectral\ndatasets, our goal is to learn robust and informative embeddings that support\ndiverse downstream applications. As a proof of concept, SpecCLIP involves\npre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed\nby contrastive alignment using the CLIP (Contrastive Language-Image\nPre-training) framework, adapted to associate spectra from different\ninstruments. This alignment is complemented by auxiliary decoders that preserve\nspectrum-specific information and enable translation (prediction) between\nspectral types, with the former achieved by maximizing mutual information\nbetween embeddings and input spectra. The result is a cross-spectrum framework\nenabling intrinsic calibration and flexible applications across instruments. We\ndemonstrate that fine-tuning these models on moderate-sized labeled datasets\nimproves adaptability to tasks such as stellar-parameter estimation and\nchemical-abundance determination. SpecCLIP also enhances the accuracy and\nprecision of parameter estimates benchmarked against external survey data.\nAdditionally, its similarity search and cross-spectrum prediction capabilities\noffer potential for anomaly detection. Our results suggest that contrastively\ntrained foundation models enriched with spectrum-aware decoders can advance\nprecision stellar spectroscopy.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "astro-ph.IM, astro-ph.SR, cs.AI, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01939v1",
    "arxiv_id": "2507.01939v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "The Roper Resonance $N^*(1440)$ in Nucleon-Nucleon Collisions and the Issue of Dibaryons",
    "authors": "Heinz Clement",
    "abstract": "In many reactions leading to excitations of the nucleon the Roper resonance\n$N^*(1440)$ can be sensed only by complex partial-wave analyses. In\nnucleon-nucleon collisions the isoscalar single-pion production as well as\nspecific two-pion production channels present the Roper excitation free of\ncompeting resonance processes at a mass of 1370 MeV and a width of 150 MeV. A\ndetailed analysis points to the formation of $N^*(1440)N$ dibaryonic systems\nduring the nucleon-nucleon collision process similar to what is known from the\n$\\Delta(1232)N$ threshold.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "hep-ex, nucl-ex, nucl-th",
    "pdf_url": "http://arxiv.org/pdf/2507.01937v1",
    "arxiv_id": "2507.01937v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "The Thin Line Between Comprehension and Persuasion in LLMs",
    "authors": "Adrian de Wynter, Tangming Yuan",
    "abstract": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.CY",
    "pdf_url": "http://arxiv.org/pdf/2507.01936v1",
    "arxiv_id": "2507.01936v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla",
    "authors": "Md Sazzadul Islam Ridoy, Sumi Akter, Md. Aminur Rahman",
    "abstract": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.AI, cs.SD, eess.AS",
    "pdf_url": "http://arxiv.org/pdf/2507.01931v1",
    "arxiv_id": "2507.01931v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations",
    "authors": "Wenhao Wang, Yanyan Li, Long Jiao, Jiawei Yuan",
    "abstract": "Large Language Models (LLMs) have revolutionized robotic autonomy, including\nUnmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential\nof LLMs for translating human instructions into executable control code for UAV\noperations. However, LLMs still face challenges from logical reasoning and\ncomplex decision-making, leading to concerns about the reliability of\nLLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop\ncontrol framework that enables reliable UAV operations powered by effective\nfeedback and refinement using two LLM modules, i.e., a Code Generator and an\nEvaluator. Our framework transforms numerical state observations from UAV\noperations into natural language trajectory descriptions to enhance the\nevaluator LLM's understanding of UAV dynamics for precise feedback generation.\nOur framework also enables a simulation-based refinement process, and hence\neliminates the risks to physical UAVs caused by incorrect code execution during\nthe refinement. Extensive experiments on UAV control tasks with different\ncomplexities are conducted. The experimental results show that our framework\ncan achieve reliable UAV operations using LLMs, which significantly outperforms\nbaseline approaches in terms of success rate and completeness with the increase\nof task complexity.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2507.01930v1",
    "arxiv_id": "2507.01930v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "evMLP: An Efficient Event-Driven MLP Architecture for Vision",
    "authors": "Zhentan Zheng",
    "abstract": "Deep neural networks have achieved remarkable results in computer vision\ntasks. In the early days, Convolutional Neural Networks (CNNs) were the\nmainstream architecture. In recent years, Vision Transformers (ViTs) have\nbecome increasingly popular. In addition, exploring applications of multi-layer\nperceptrons (MLPs) has provided new perspectives for research into vision model\narchitectures. In this paper, we present evMLP accompanied by a simple\nevent-driven local update mechanism. The proposed evMLP can independently\nprocess patches on images or feature maps via MLPs. We define changes between\nconsecutive frames as \"events\". Under the event-driven local update mechanism,\nevMLP selectively processes patches where events occur. For sequential image\ndata (e.g., video processing), this approach improves computational performance\nby avoiding redundant computations. Through ImageNet image classification\nexperiments, evMLP attains accuracy competitive with state-of-the-art models.\nMore significantly, experimental results on multiple video datasets demonstrate\nthat evMLP reduces computational cost via its event-driven local update\nmechanism while maintaining output consistency with its non-event-driven\nbaseline. The code and trained models are available at\nhttps://github.com/i-evi/evMLP.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01927v1",
    "arxiv_id": "2507.01927v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "A Survey on Vision-Language-Action Models: An Action Tokenization Perspective",
    "authors": "Yifan Zhong, Fengshuo Bai, Shaofei Cai, Xuchuan Huang, Zhang Chen, Xiaowei Zhang, Yuanfei Wang, Shaoyang Guo, Tianrui Guan, Ka Nam Lui, Zhiquan Qi, Yitao Liang, Yuanpei Chen, Yaodong Yang",
    "abstract": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof \\textit{action tokens} that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.RO",
    "pdf_url": "http://arxiv.org/pdf/2507.01925v1",
    "arxiv_id": "2507.01925v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Decision-oriented Text Evaluation",
    "authors": "Yu-Shiang Huang, Chuan-Ju Wang, Chung-Chi Chen",
    "abstract": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01923v1",
    "arxiv_id": "2507.01923v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Efficient stochastic simulation of gene regulatory networks using hybrid models of transcriptional bursting",
    "authors": "Mathilde Gaillard, Ulysse Herbach",
    "abstract": "Single-cell data reveal the presence of biological stochasticity between\ncells of identical genome and environment, in particular highlighting the\ntranscriptional bursting phenomenon. To account for this property, gene\nexpression may be modeled as a continuous-time Markov chain where biochemical\nspecies are described in a discrete way, leading to Gillespie's stochastic\nsimulation algorithm (SSA) which turns out to be computationally expensive for\nrealistic mRNA and protein copy numbers. Alternatively, hybrid models based on\npiecewise-deterministic Markov processes (PDMPs) offer an effective compromise\nfor capturing cell-to-cell variability, but their simulation remains limited to\nspecialized mathematical communities. With a view to making them more\naccessible, we present here a simple simulation method that is reminiscent of\nSSA, while allowing for much lower computational cost. We detail the algorithm\nfor a bursty PDMP describing an arbitrary number of interacting genes, and\nprove that it simulates exact trajectories of the model. As an illustration, we\nuse the algorithm to simulate a two-gene toggle switch: this example highlights\nthe fact that bimodal distributions as observed in real data are not explained\nby transcriptional bursting per se, but rather by distinct burst frequencies\nthat may emerge from interactions between genes.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "q-bio.MN, math.PR",
    "pdf_url": "http://arxiv.org/pdf/2507.01922v1",
    "arxiv_id": "2507.01922v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks",
    "authors": "Yang Li, Youssef Emad, Karthik Padthe, Jack Lanchantin, Weizhe Yuan, Thao Nguyen, Jason Weston, Shang-Wen Li, Dong Wang, Ilia Kulikov, Xian Li",
    "abstract": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01921v1",
    "arxiv_id": "2507.01921v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models",
    "authors": "Chengao Li, Hanyu Zhang, Yunkun Xu, Hongyan Xue, Xiang Ao, Qing He",
    "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.AI, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01915v1",
    "arxiv_id": "2507.01915v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "On the influence of reference sample properties on magnetic force microscopy calibrations",
    "authors": "Baha Sakar, Christopher Habenschaden, Sibylle Sievers, Hans Werner Schumacher",
    "abstract": "Magnetic force microscopy (MFM) allows the characterization of magnetic stray\nfield distributions with high sensitivity and spatial resolution. Based on a\nsuitable calibration procedure, MFM can also yield quantitative magnetic field\nvalues. This process typically involves measuring a reference sample to\ndetermine the distribution of the tip's stray field or stray field gradient at\nthe sample surface. This distribution is called the tip transfer function (TTF)\nand is derived through regularized deconvolution in Fourier space. The\nproperties of the reference sample and the noise characteristics of the\ndetection system significantly influence the derived TTF, thereby limiting its\nvalidity range. In a recent study, the tip stray field distribution, and hence\nthe TTF, of an MFM tip was independently measured in real space using a\nnitrogen vacancy center as a quantum sensor, revealing considerable\ndiscrepancies with the reference-sample-based TTF. Here, we analyze the\ninfluence of the feature distribution of the reference sample and the MFM\nmeasurement parameters on the resulting TTF. We explain the observed\ndifferences between quantum-calibrated stray field distributions and the\nclassical approach by attributing them to a loss of information due to missing\nor suppressed spectral components. Furthermore, we emphasize the importance of\nthe spectral coverage of the TTF. Our findings indicate that for high-quality\nreconstruction of the stray field of a sample under test (SUT), it is more\ncritical to ensure a strong overlap of frequency components between the\nreference sample and the SUT than to achieve an accurate real-space\nreconstruction of the tip stray field distribution.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cond-mat.mes-hall",
    "pdf_url": "http://arxiv.org/pdf/2507.01911v1",
    "arxiv_id": "2507.01911v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning",
    "authors": "Qingdong He, Xueqin Chen, Chaoyi Wang, Yanjie Pan, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xiangtai Li, Jiangning Zhang",
    "abstract": "Instruction-based image editing (IIE) has advanced rapidly with the success\nof diffusion models. However, existing efforts primarily focus on simple and\nexplicit instructions to execute editing operations such as adding, deleting,\nmoving, or swapping objects. They struggle to handle more complex implicit\nhypothetical instructions that require deeper reasoning to infer plausible\nvisual changes and user intent. Additionally, current datasets provide limited\nsupport for training and evaluating reasoning-aware editing capabilities.\nArchitecturally, these methods also lack mechanisms for fine-grained detail\nextraction that support such reasoning. To address these limitations, we\npropose Reason50K, a large-scale dataset specifically curated for training and\nevaluating hypothetical instruction reasoning image editing, along with\nReasonBrain, a novel framework designed to reason over and execute implicit\nhypothetical instructions across diverse scenarios. Reason50K includes over 50K\nsamples spanning four key reasoning scenarios: Physical, Temporal, Causal, and\nStory reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)\nfor editing guidance generation and a diffusion model for image synthesis,\nincorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture\ndetailed visual and textual semantics essential for supporting instruction\nreasoning. To mitigate the semantic loss, we further introduce a Cross-Modal\nEnhancer (CME) that enables rich interactions between the fine-grained cues and\nMLLM-derived features. Extensive experiments demonstrate that ReasonBrain\nconsistently outperforms state-of-the-art baselines on reasoning scenarios\nwhile exhibiting strong zero-shot generalization to conventional IIE tasks. Our\ndataset and code will be released publicly.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CV",
    "pdf_url": "http://arxiv.org/pdf/2507.01908v1",
    "arxiv_id": "2507.01908v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Attosecond Control and Measurement of Chiral Photoionisation Dynamics",
    "authors": "Meng Han, Jia-Bao Ji, Alexander Blech, R. Esteban Goetz, Corbin Allison, Loren Greenman, Christiane P. Koch, Hans Jakob Wörner",
    "abstract": "Many chirality-sensitive light-matter interactions are governed by chiral\nelectron dynamics. Therefore, the development of advanced technologies\nharnessing chiral phenomena would critically benefit from measuring and\ncontrolling chiral electron dynamics on their natural attosecond time scales.\nSuch endeavors have so far been hampered by the lack of characterized\ncircularly polarized attosecond pulses, an obstacle that has recently been\novercome (Han et al. Optica 10 (2023) 1044-1052, Han et al. Nature Physics 19\n(2023) 230-236). In this article, we introduce chiroptical spectroscopy with\nattosecond pulses and demonstrate attosecond coherent control over\nphotoelectron circular dichroism (PECD) (Goetz et al. Physical Review Letters\n122 (2019) 013204, Goetz et al. arXiv:2104.07522), as well as the measurement\nof chiral asymmetries in the forward-backward and angle-resolved\nphotoionisation delays of chiral molecules. We show that co-rotating attosecond\nand near-infrared pulses can nearly double the PECD and even change its sign\ncompared to single-photon ionisation. We demonstrate that chiral\nphotoionisation delays depend on both polar and azimuthal angles of\nphotoemission in the light-propagation frame, requiring three-dimensional\nmomentum resolution. We measure forward-backward chiral-sensitive delays of up\nto 120 as and polar-angle-resolved photoionisation delays up to 240 as, which\ninclude an asymmmetry of $\\sim$60 as originating from chirality in the\ncontinuum-continuum transitions. Attosecond chiroptical spectroscopy opens the\ndoor to quantitatively understanding and controlling the dynamics of chiral\nmolecules on the electronic time scale.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "physics.chem-ph",
    "pdf_url": "http://arxiv.org/pdf/2507.01906v1",
    "arxiv_id": "2507.01906v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
    "authors": "Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che",
    "abstract": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.AI",
    "pdf_url": "http://arxiv.org/pdf/2507.01903v1",
    "arxiv_id": "2507.01903v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "High-Layer Attention Pruning with Rescaling",
    "authors": "Songtao Liu, Peng Liu",
    "abstract": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01900v1",
    "arxiv_id": "2507.01900v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "The Frobenius number corresponding to the squares of three consecutive Fibonacci numbers: comparison of three algorithmic processes",
    "authors": "Aureliano M. Robles-Pérez, José Carlos Rosales",
    "abstract": "We compute the Frobenius number for numerical semigroups generated by the\nsquares of three consecutive Fibonacci numbers. We achieve this by using and\ncomparing three distinct algorithmic approaches: those developed by Ram\\'irez\nAlfons\\'in and R{\\o}dseth ([15]), Rosales and Garc\\'ia-S\\'anchez ([20]), and\nTripathi ([26]).",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "math.NT, math.GR, 11D07, 20M13, 20M14",
    "pdf_url": "http://arxiv.org/pdf/2507.01898v1",
    "arxiv_id": "2507.01898v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "From Photospheric Footpoint Motion to Plasmoid Ejection: A Two-Stage Reconnection Process in a Small-scale Chromospheric Jet",
    "authors": "Zehao Tang, Yuandeng Shen, Chengrui Zhou, Surui Yao, Dongxu Liu, Xiaobo Li",
    "abstract": "Using high spatiotemporal resolution, multi-wavelength observations from the\nNew Vacuum Solar Telescope (NVST) and the Solar Dynamics Observatory (SDO), we\npresent a detailed analysis of a small-scale chromospheric jet driven by\nplasmoid-mediated magnetic reconnection. Our results reveal that the entire\nprocess is governed by the dynamic evolution of photospheric magnetic\nfootpoints, which proceeds in two distinct stages. An initial separating motion\nof the footpoints corresponds to a mild reconnection phase, characterized by a\nshort current sheet and the eruption of a cool H$\\alpha$ jet. Subsequently, a\nconverging motion of the footpoints triggers an intense reconnection phase.\nDuring this intense stage, the current sheet rapidly elongates, and the\nresulting decrease in its aspect ratio initiates a tearing-mode instability,\nforming a plasmoid. The appearance of this plasmoid mediates the onset of fast\nmagnetic reconnection, which produces a hot EUV jet and is concurrent with\nsignificant magnetic flux cancellation. We interpret this cancellation as the\nsubmergence of newly formed, post-reconnection loops. Furthermore, we identify\na distinct, high-temperature plasma blob in the jet spire, significantly hotter\nthan the surrounding jet plasma. We attribute this feature to a secondary\nheating process, likely caused by reconnection between the upward-propagating\nplasmoid and the overlying magnetic cusp structure. These observations provide\na comprehensive, observationally driven picture (from the initial photospheric\ntriggers to the multi-stage, plasmoid-mediated reconnection) that forms\nchromospheric jets, highlighting the critical role of footpoint motions in\nsolar atmospheric dynamics.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "astro-ph.SR",
    "pdf_url": "http://arxiv.org/pdf/2507.01896v1",
    "arxiv_id": "2507.01896v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "$E_6$ Unification of Freeze-In Dark Matter and Leptogenesis",
    "authors": "Adeela Afzal, Rishav Roshan",
    "abstract": "We investigate the possibility of \\emph{freeze-in} dark matter production and\nbaryogenesis in an $E_6$ extension of the Standard Model, featuring a residual\n$U(1)_{\\psi'}$ gauge symmetry. This symmetry arises from a linear combination\nof $U(1)\\chi$ and $U(1)_{\\psi}$, both of which are subgroups of the $E_6$. The\nspontaneous breaking of $U(1)_{\\psi'}$ symmetry governs the dynamics of a\nsinglet fermion, which naturally serves as a freeze-in dark matter candidate.\nThe dark matter mass arises from dimension-five operators, and a discrete\nsymmetry ensures their stability. We show that freeze-in production from scalar\ndecay can yield the correct relic abundance for dark matter masses between few\nMeV to a few hundred GeV. Simultaneously, heavy right-handed neutrinos generate\nlight neutrino masses via the type-I seesaw and produce the observed baryon\nasymmetry via leptogenesis.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "hep-ph, astro-ph.CO, gr-qc, hep-th",
    "pdf_url": "http://arxiv.org/pdf/2507.01895v1",
    "arxiv_id": "2507.01895v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Perceptual Ratings Predict Speech Inversion Articulatory Kinematics in Childhood Speech Sound Disorders",
    "authors": "Nina R. Benway, Saba Tabatabaee, Dongliang Wang, Benjamin Munson, Jonathan L. Preston, Carol Espy-Wilson",
    "abstract": "Purpose: This study evaluated whether articulatory kinematics, inferred by\nArticulatory Phonology speech inversion neural networks, aligned with\nperceptual ratings of /r/ and /s/ in the speech of children with speech sound\ndisorders.\n  Methods: Articulatory Phonology vocal tract variables were inferred for 5,961\nutterances from 118 children and 3 adults, aged 2.25-45 years. Perceptual\nratings were standardized using the novel 5-point PERCEPT Rating Scale and\ntraining protocol. Two research questions examined if the articulatory patterns\nof inferred vocal tract variables aligned with the perceptual error category\nfor the phones investigated (e.g., tongue tip is more anterior in dentalized\n/s/ productions than in correct /s/). A third research question examined if\ngradient PERCEPT Rating Scale scores predicted articulatory proximity to\ncorrect productions.\n  Results: Estimated marginal means from linear mixed models supported 17 of 18\n/r/ hypotheses, involving tongue tip and tongue body constrictions. For /s/,\nestimated marginal means from a second linear mixed model supported 7 of 15\nhypotheses, particularly those related to the tongue tip. A third linear mixed\nmodel revealed that PERCEPT Rating Scale scores significantly predicted\narticulatory proximity of errored phones to correct productions.\n  Conclusion: Inferred vocal tract variables differentiated category and\nmagnitude of articulatory errors for /r/, and to a lesser extent for /s/,\naligning with perceptual judgments. These findings support the clinical\ninterpretability of speech inversion vocal tract variables and the PERCEPT\nRating Scale in quantifying articulatory proximity to the target sound,\nparticularly for /r/.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "eess.AS",
    "pdf_url": "http://arxiv.org/pdf/2507.01888v1",
    "arxiv_id": "2507.01888v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants",
    "authors": "Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou",
    "abstract": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.CL",
    "pdf_url": "http://arxiv.org/pdf/2507.01887v1",
    "arxiv_id": "2507.01887v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs",
    "authors": "Niccolò McConnell, Pardeep Vasudev, Daisuke Yamada, Daryl Cheng, Mehran Azimbagirad, John McCabe, Shahab Aslani, Ahmed H. Shahin, Yukun Zhou, The SUMMIT Consortium, Andre Altmann, Yipeng Hu, Paul Taylor, Sam M. Janes, Daniel C. Alexander, Joseph Jacob",
    "abstract": "Low-dose computed tomography (LDCT) imaging employed in lung cancer screening\n(LCS) programs is increasing in uptake worldwide. LCS programs herald a\ngenerational opportunity to simultaneously detect cancer and non-cancer-related\nearly-stage lung disease. Yet these efforts are hampered by a shortage of\nradiologists to interpret scans at scale. Here, we present TANGERINE, a\ncomputationally frugal, open-source vision foundation model for volumetric LDCT\nanalysis. Designed for broad accessibility and rapid adaptation, TANGERINE can\nbe fine-tuned off the shelf for a wide range of disease-specific tasks with\nlimited computational resources and training data. Relative to models trained\nfrom scratch, TANGERINE demonstrates fast convergence during fine-tuning,\nthereby requiring significantly fewer GPU hours, and displays strong label\nefficiency, achieving comparable or superior performance with a fraction of\nfine-tuning data. Pretrained using self-supervised learning on over 98,000\nthoracic LDCTs, including the UK's largest LCS initiative to date and 27 public\ndatasets, TANGERINE achieves state-of-the-art performance across 14 disease\nclassification tasks, including lung cancer and multiple respiratory diseases,\nwhile generalising robustly across diverse clinical centres. By extending a\nmasked autoencoder framework to 3D imaging, TANGERINE offers a scalable\nsolution for LDCT analysis, departing from recent closed, resource-intensive\nmodels by combining architectural simplicity, public availability, and modest\ncomputational requirements. Its accessible, open-source lightweight design lays\nthe foundation for rapid integration into next-generation medical imaging tools\nthat could transform LCS initiatives, allowing them to pivot from a singular\nfocus on lung cancer detection to comprehensive respiratory disease management\nin high-risk populations.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "eess.IV, cs.CV, cs.LG",
    "pdf_url": "http://arxiv.org/pdf/2507.01881v1",
    "arxiv_id": "2507.01881v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  },
  {
    "title": "Joint Power Control and Precoding for Cell-Free Massive MIMO Systems With Sparse Multi-Dimensional Graph Neural Networks",
    "authors": "Yukun Ma, Jiayi Zhang, Ziheng Liu, Guowei Shi, Bo Ai",
    "abstract": "Cell-free massive multiple-input multiple-output (CF mMIMO) has emerged as a\nprominent candidate for future networks due to its ability to significantly\nenhance spectral efficiency by eliminating inter-cell interference. However,\nits practical deployment faces considerable challenges, such as high\ncomputational complexity and the optimization of its complex processing. To\naddress these challenges, this correspondence proposes a framework based on a\nsparse multi-dimensional graph neural network (SP-MDGNN), which sparsifies the\nconnections between access points (APs) and user equipments (UEs) to\nsignificantly reduce computational complexity while maintaining high\nperformance. In addition, the weighted minimum mean square error (WMMSE)\nalgorithm is introduced as a comparative method to further analyze the\ntrade-off between performance and complexity. Simulation results demonstrate\nthat the sparse method achieves an optimal balance between performance and\ncomplexity, significantly reducing the computational complexity of the original\nMDGNN method while incurring only a slight performance degradation, providing\ninsights for the practical deployment of CF mMIMO systems in large-scale\nnetwork.",
    "published_date": "2025-07-02",
    "year": 2025,
    "categories": "cs.IT, eess.SP, math.IT",
    "pdf_url": "http://arxiv.org/pdf/2507.01876v1",
    "arxiv_id": "2507.01876v1",
    "source": "arxiv",
    "citation_count": 0,
    "journal": "arXiv preprint"
  }
]